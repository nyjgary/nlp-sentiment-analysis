{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time \n",
    "import os\n",
    "from os import listdir \n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import partial \n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim import SGD \n",
    "from torch.optim import RMSprop\n",
    "from ast import literal_eval\n",
    "from datetime import datetime\n",
    "pd.set_option('max_colwidth',100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to load reviews from directories \n",
    "\n",
    "def load_single_review(fdir, fname): \n",
    "    \"\"\" Takes as input file directory and file name of a single review, returns review as string \"\"\"\n",
    "    fpath = fdir + '/' + fname \n",
    "    with open(fpath, 'r') as f: \n",
    "        review = f.read()\n",
    "        return review \n",
    "    \n",
    "def load_dir_reviews(fdir): \n",
    "    \"\"\" Takes as input file directory where reviews are stored, returns them as a list of review strings \"\"\"\n",
    "    fnames = [f for f in listdir(fdir)]\n",
    "    reviews = [load_single_review(fdir, fname) for fname in fnames]\n",
    "    return reviews\n",
    "\n",
    "def combine_data(neg_reviews, pos_reviews): \n",
    "    \"\"\" Combines lists of negative and positive reviews, returns a combined dataset comprising reviews and labels \"\"\"\n",
    "    neg_with_labels = [(review, 0) for review in neg_reviews] \n",
    "    pos_with_labels = [(review, 1) for review in pos_reviews]\n",
    "    combined = neg_with_labels + pos_with_labels\n",
    "    combined = random.sample(combined, len(combined))\n",
    "    reviews = [comb[0] for comb in combined]\n",
    "    labels = [comb[1] for comb in combined]\n",
    "    return reviews, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews into lists \n",
    "train_val_neg = load_dir_reviews('aclImdb/train/neg')\n",
    "train_val_pos = load_dir_reviews('aclImdb/train/pos')\n",
    "test_neg = load_dir_reviews('aclImdb/test/neg')\n",
    "test_pos = load_dir_reviews('aclImdb/test/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split train into train vs. validation sets \n",
    "train_split = int(20000 / 2) \n",
    "train_neg = train_val_neg[:train_split]\n",
    "train_pos = train_val_pos[:train_split]\n",
    "val_neg = train_val_neg[train_split:]\n",
    "val_pos = train_val_pos[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# combine pos and neg reviews to get unified datasets \n",
    "train_data, train_labels = combine_data(train_neg, train_pos)\n",
    "val_data, val_labels = combine_data(val_neg, val_pos)\n",
    "test_data, test_labels = combine_data(test_neg, test_pos)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put various optimization algorithms in a dictionary for ease of recording results + running experiments\n",
    "optim_algos = {'Adam': Adam, 'SGD': SGD, 'RMSprop': RMSprop}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews \n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lower_case(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed]\n",
    "\n",
    "def tokenize_dataset(dataset, processing_func):\n",
    "    \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "        a list comprising all the tokens from the dataset \"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "                                               disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = processing_func(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset\n",
    "\n",
    "def save_to_pickle(item_to_save, filename): \n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    with open(filename, \"wb\") as f: \n",
    "        pkl.dump(item_to_save, f)\n",
    "    \n",
    "def tokenize_dataset_to_disk(dataset, processing_func, filename): \n",
    "    \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "    start_time = time.time() \n",
    "    token_dataset = tokenize_dataset(dataset, processing_func)\n",
    "    save_to_pickle(token_dataset, filename)\n",
    "    time_elapsed = (time.time() - start_time) / 60.0 \n",
    "    print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(filename, time_elapsed))\n",
    "    return token_dataset\n",
    "    \n",
    "def tokenize_datasets_to_disk(train_data, val_data, test_data, processing_func, folder_name): \n",
    "    \"\"\" Tokenizes train, val, test datasets and save as pickle to data subfolder \n",
    "        Also returns tokenized datasets\n",
    "    \"\"\"\n",
    "    train_data_tokens = tokenize_dataset_to_disk(train_data, processing_func, \n",
    "                                                 \"data/{}/train_data_tokens.p\".format(folder_name))\n",
    "    val_data_tokens = tokenize_dataset_to_disk(val_data, processing_func, \n",
    "                                               \"data/{}/val_data_tokens.p\".format(folder_name))    \n",
    "    test_data_tokens = tokenize_dataset_to_disk(test_data, processing_func, \n",
    "                                                \"data/{}/test_data_tokens.p\".format(folder_name))\n",
    "    return train_data_tokens, val_data_tokens, test_data_tokens\n",
    "\n",
    "def load_tokens_from_disk(folder_name): \n",
    "    \"\"\" Loads train/val/test tokens from disk for a given set of tokenization scheme, denoted by folder name\"\"\"\n",
    "    train_data_tokens = pkl.load(open(\"data/{}/train_data_tokens.p\".format(folder_name), \"rb\"))\n",
    "    val_data_tokens = pkl.load(open(\"data/{}/val_data_tokens.p\".format(folder_name), \"rb\"))\n",
    "    test_data_tokens = pkl.load(open(\"data/{}/test_data_tokens.p\".format(folder_name), \"rb\"))\n",
    "    return train_data_tokens, val_data_tokens, test_data_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0916e77a71db45b09bfdab435cfd6f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/train_data_tokens.p in 0.7 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076c4323f78943a7a993132c5f3a03ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f520f472ffc84985aea0cbf18df9ef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/test_data_tokens.p in 0.8 minutes\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens, val_data_tokens, test_data_tokens = tokenize_datasets_to_disk(\n",
    "    train_data, val_data, test_data, processing_func=lower_case, folder_name='lowercase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439615\n"
     ]
    }
   ],
   "source": [
    "# load saved tokens \n",
    "train_data_tokens, val_data_tokens, test_data_tokens = load_tokens_from_disk(folder_name='lowercase')\n",
    "all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "\n",
    "# double check \n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from 10000 most common tokens in the training set \n",
    "\n",
    "PAD_IDX=0\n",
    "UNK_IDX=1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size=10000, pad_idx=PAD_IDX, unk_idx=UNK_IDX): \n",
    "    \"\"\" Takes list of all tokens and returns:\n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    token2id['<pad>'] = pad_idx\n",
    "    token2id['<unk>'] = unk_idx\n",
    "    return token2id, id2token \n",
    "    \n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5342 ; token penny\n",
      "Token penny; token id 5342\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset \n",
    "\n",
    "def token2index_dataset(tokens_data, token2id, unk_idx=UNK_IDX): \n",
    "    \"\"\" Converts data from word tokens to token indices \"\"\"\n",
    "    indices_data = []\n",
    "    for datum in tokens_data: \n",
    "        index_list = [token2id[token] if token in token2id else unk_idx for token in datum]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data \n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "test_data_indices = token2index_dataset(test_data_tokens, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# check size of data \n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['series', 'three-', 'blackadder', 'the', 'third', '\"', 'if', 'you', 'want', 'something', 'done', 'properly', ',', 'kill', 'baldrick', 'before', 'you', 'start', '\"', 'hot', 'on', 'the', 'heels', 'of', 'the', 'second', 'series', 'the', 'show', 'returned', 'with', 'the', 'current', 'owner', 'of', 'the', 'famous', 'name', 'down', 'on', 'his', 'luck', 'and', 'in', 'service', 'as', 'butler', 'to', 'the', 'prince', 'regent', ',', 'a', 'vain', 'and', 'stupid', 'foil', 'for', 'blackadders', 'venom', ',', 'played', 'by', 'hugh', 'laurie', '.', 'baldrick', 'is', 'still', 'in', 'tow', 'as', 'the', 'other', 'piece', 'of', 'the', 'comedic', 'jigsaw', '.', 'the', 'format', 'is', 'similar', 'to', 'the', 'previous', 'show', ',', 'after', 'all', 'now', 'they', 'had', 'found', 'the', 'winning', 'formula', 'why', 'change', 'things', '.', 'we', 'see', 'blackadder', 'trying', 'to', 'get', 'rich', 'off', 'of', 'the', 'back', 'of', 'the', 'gullible', 'regent', 'in', 'many', 'more', 'ingenious', 'ways', ',', 'trying', 'to', 'make', 'bladrick', 'an', 'm.p.or', 'trying', 'to', 'woe', 'a', 'suitable', 'bride', 'for', 'the', 'prince', '.', 'in', 'many', 'ways', 'this', 'is', 'one', 'of', 'the', 'most', 'accurate', 'of', 'the', 'series', 'historically', ',', 'the', 'prince', 'regent', 'did', 'take', 'control', 'of', 'the', 'throne', 'during', 'his', 'fathers', 'bout', 'of', 'madness', 'and', 'some', 'of', 'the', 'characters', 'lampooned', 'tell', 'a', 'lot', 'about', 'the', 'times', '.', 'samuel', 'johnson', ',', 'william', 'pit', 'and', 'wellington', 'all', 'pass', 'through', 'the', 'events', 'and', 'all', 'manage', 'to', 'steal', 'their', 'scenes', ',', 'not', 'an', 'easy', 'thing', 'with', 'such', 'a', 'stellar', 'cast']\n",
      "[217, 1, 1, 2, 891, 15, 57, 27, 196, 158, 236, 2973, 3, 514, 1, 173, 27, 392, 15, 899, 26, 2, 6134, 7, 2, 353, 217, 2, 128, 3598, 22, 2, 2029, 2255, 7, 2, 772, 414, 199, 26, 34, 2067, 5, 11, 2501, 20, 4803, 8, 2, 2160, 1, 3, 6, 6503, 5, 403, 7064, 21, 1, 1, 3, 270, 41, 3696, 5957, 4, 1, 9, 143, 11, 9287, 20, 2, 97, 432, 7, 2, 1708, 1, 4, 2, 2876, 9, 746, 8, 2, 971, 128, 3, 116, 38, 175, 40, 77, 272, 2, 1512, 2145, 153, 646, 207, 4, 81, 78, 1, 278, 8, 92, 1052, 140, 7, 2, 157, 7, 2, 1, 1, 11, 125, 63, 6229, 806, 3, 278, 8, 107, 1, 43, 1, 278, 8, 1, 6, 4337, 3286, 21, 2, 2160, 4, 11, 125, 806, 13, 9, 37, 7, 2, 102, 1814, 7, 2, 217, 4804, 3, 2, 2160, 1, 82, 212, 1139, 7, 2, 8019, 336, 34, 5661, 1, 7, 2877, 5, 59, 7, 2, 119, 1, 377, 6, 187, 52, 2, 232, 4, 7329, 2915, 3, 986, 5513, 5, 8487, 38, 1353, 160, 2, 699, 5, 38, 1987, 8, 2107, 80, 155, 3, 31, 43, 798, 169, 22, 152, 6, 4190, 190]\n"
     ]
    }
   ],
   "source": [
    "# check tokenization of dataset \n",
    "print(train_data_tokens[0])\n",
    "print(train_data_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class and collate function for data loader \n",
    "\n",
    "class MovieReviewsDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list, max_sentence_length): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of movie review tokens and a list of labels \n",
    "        \"\"\"\n",
    "        self.data_list = data_list \n",
    "        self.label_list = label_list \n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.data_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, length of list, and label of the data point\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:self.max_sentence_length]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "def collate_func(max_sentence_length, batch): \n",
    "    \"\"\" \n",
    "    Customized function for DataLoader that dynamically pads the batch so that the data have the same length\n",
    "    Note that this takes max_sentence_length as a first argument to be prefilled with a partial function later \n",
    "        to facilitate hyperparameter tuning \n",
    "    \"\"\"\n",
    "    data_list = [] \n",
    "    label_list = [] \n",
    "    length_list = [] \n",
    "    \n",
    "    for datum in batch:         \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        # pad data before appending \n",
    "        padded_vec = np.pad(array = np.array(datum[0]), \n",
    "                            pad_width = ((0, max_sentence_length - datum[1])), \n",
    "                            mode = 'constant', constant_values = 0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture and helper methods \n",
    "\n",
    "class BagOfWords(nn.Module): \n",
    "    \"\"\" \n",
    "    BagOfWords classification model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim): \n",
    "        \"\"\" \n",
    "        @param vocab_size: size of the vocabulary \n",
    "        @param emd_dim: size of the word embedding \n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "        \n",
    "    def forward(self, data, length): \n",
    "        \"\"\" \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a review\n",
    "            that is represented using n-gram index. Note that they are padded to have the same length. \n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (i.e. non-padded)\n",
    "            length of each sentence in the data \n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "        \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to train and test model \n",
    "\n",
    "def test_model(loader, model): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    model.eval() \n",
    "    \n",
    "    for data_batch, length_batch, label_batch in loader: \n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predictions = outputs.max(1, keepdim=True)[1]    \n",
    "        total += label_batch.size(0)\n",
    "        correct += predictions.eq(label_batch.view_as(predictions)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, optimizer, train_loader, val_loader, num_epochs, print_intermediate=True):  \n",
    "    \"\"\"\n",
    "    Trains model on data from train_loader and evaluates on data from val_loader for num_epochs \n",
    "    Returns results as a dictionary comprising epoch, train accuracy, and validation accuracy \n",
    "    \"\"\"\n",
    "    # train and validate \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    results = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i % 100 == 0 or ((epoch==num_epochs-1) & (i==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + i / len(train_loader)\n",
    "                result['train_acc'] = test_model(train_loader, model)\n",
    "                result['val_acc'] = test_model(val_loader, model)       \n",
    "                results.append(result)\n",
    "\n",
    "                if print_intermediate: \n",
    "                    print('Epoch: {:.2f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'.format(\n",
    "                        result['epoch'], result['train_acc'], result['val_acc']))\n",
    "    \n",
    "    return results \n",
    "\n",
    "# from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# def train_and_eval(model, optimizer, train_loader, val_loader, num_epochs, print_results=True, scheduler=None):  \n",
    "#     \"\"\"\n",
    "#     Trains model on data from train_loader and evaluates on data from val_loader for num_epochs \n",
    "#     Returns results as a dictionary comprising epoch, train accuracy, and validation accuracy \n",
    "#     \"\"\"\n",
    "#     # train and validate \n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "#     scheduler = LambdaLR(optimizer, lr_lambda=, last_epoch=-1)\n",
    "#     results = []\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "#             model.train()\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(data_batch, length_batch)\n",
    "#             loss = criterion(outputs, label_batch)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             # validate every 100 iterations\n",
    "#             if i % 100 == 0 or ((epoch==num_epochs-1) & (i==len(train_loader)-1)):\n",
    "#                 result = {} \n",
    "#                 result['epoch'] = epoch + i / len(train_loader)\n",
    "#                 result['train_acc'] = test_model(train_loader, model)\n",
    "#                 result['val_acc'] = test_model(val_loader, model)       \n",
    "#                 results.append(result)\n",
    "\n",
    "#                 if print_results: \n",
    "#                     print('Epoch: {:.2f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'.format(\n",
    "#                         result['epoch'], result['train_acc'], result['val_acc']))\n",
    "#         if scheduler is not None: \n",
    "#             scheduler.step()\n",
    "    \n",
    "#     return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to save results to and load results from a pkl logfile \n",
    "\n",
    "RESULTS_LOG = 'experiment_results/experiment_results_log.pkl'\n",
    "\n",
    "def append_to_log(hyperparams, results, runtime, experiment_name, filename=RESULTS_LOG): \n",
    "    \"\"\" Appends results and details of a single experiment to a log file \"\"\"\n",
    "    \n",
    "    # create directory if doesn't already exist \n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "        \n",
    "    # store experiment details in a dictionary \n",
    "    new_result = {'experiment_name': experiment_name, 'hyperparams': hyperparams, 'results': results, \n",
    "                  'runtime': runtime, 'dt_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "    \n",
    "    # if log already exists, append to log \n",
    "    try: \n",
    "        results_log = pkl.load(open(filename, \"rb\"))\n",
    "        results_log.append(new_result)\n",
    "\n",
    "    # if log doesn't exists, initialize first result as the log \n",
    "    except (OSError, IOError) as e:\n",
    "        results_log = [new_result]\n",
    "    \n",
    "    # save to pickle \n",
    "    pkl.dump(results_log, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_log(experiment_name=None, filename=RESULTS_LOG): \n",
    "    \"\"\" Loads experiment log, with option to filter for a specific experiment_name \"\"\"\n",
    "    \n",
    "    results_log = pkl.load(open(filename, \"rb\"))\n",
    "    \n",
    "    if experiment_name is not None: \n",
    "        results_log = [r for r in results_log if r['experiment_name'] == experiment_name]\n",
    "        \n",
    "    return results_log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_tokens, val_tokens, train_labels, val_labels, max_sentence_length, max_vocab_size, \n",
    "                   emb_dim, optim_algo, learning_rate, num_epochs, batch_size=32, \n",
    "                   experiment_name='Not Specified', token_scheme='Default', \n",
    "                   save_to_log=True, print_summary=True, print_intermediate=False):  \n",
    "    \n",
    "    \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "        Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "        lag time in generating tokens.  \n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time() \n",
    "    \n",
    "    # build vocab based on max_vocab_size specified \n",
    "    all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    \n",
    "    # convert tokens to token indices \n",
    "    train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "    val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "    # instantiate PyTorch Dataset object \n",
    "    train_dataset = MovieReviewsDataset(train_data_indices, train_labels, max_sentence_length)\n",
    "    val_dataset = MovieReviewsDataset(val_data_indices, val_labels, max_sentence_length)\n",
    "    \n",
    "    # create PyTorch DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                               collate_fn=partial(collate_func, max_sentence_length))\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                             collate_fn=partial(collate_func, max_sentence_length))\n",
    "    \n",
    "    # instantiate model and optimizer \n",
    "    model = BagOfWords(len(token2id), emb_dim)\n",
    "    optimizer = optim_algos[optim_algo](params=model.parameters(), lr=learning_rate) \n",
    "#    optim = optimizer(params=model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # train and evaluate \n",
    "    results = train_and_eval(model, optimizer, train_loader, val_loader, num_epochs, print_intermediate)\n",
    "    \n",
    "    # store, print, and save results \n",
    "    runtime = time.time() - start_time \n",
    "    hyperparams = {'max_sentence_length': max_sentence_length, 'max_vocab_size': max_vocab_size, \n",
    "                   'emb_dim': emb_dim, 'optim_algo': optim_algo, 'learning_rate': learning_rate, \n",
    "                   'num_epochs': num_epochs, 'token_scheme': token_scheme}\n",
    "    if save_to_log: \n",
    "        append_to_log(hyperparams, results, runtime, experiment_name)\n",
    "    if print_summary: \n",
    "        print(\"Experiment completed in {} seconds with {}% validation accuracy.\".format(\n",
    "            int(runtime), pd.DataFrame.from_dict(results)['val_acc'].max()))\n",
    "        \n",
    "    return results, hyperparams, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to summarize, evaluate, and plot results \n",
    "\n",
    "def summarize_results(results_log, exclude_cols=['hyperparams', 'results']): \n",
    "    \"\"\" Summarizes results_log (list) into a dataframe, splitting hyperparameters string into columns, and reducing \n",
    "        the val_acc dict into the best validation accuracy obtained amongst all the epochs logged \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results_log)\n",
    "    results_df = pd.concat([results_df, results_df['hyperparams'].apply(pd.Series)], axis=1)\n",
    "    results_df['val_acc'] = results_df['results'].apply(lambda d: pd.DataFrame.from_dict(d)['val_acc'].max())\n",
    "    all_cols = ['experiment_name', 'token_scheme', 'num_epochs', 'runtime', 'max_sentence_length', 'max_vocab_size', \n",
    "                'emb_dim', 'optim_algo', 'learning_rate', 'val_acc', 'hyperparams', 'results']\n",
    "    output_cols = [col for col in all_cols if col not in exclude_cols]\n",
    "    return results_df[output_cols] \n",
    "\n",
    "def plot_multiple_learning_curves(results_log, plot_variable): \n",
    "    \"\"\" Plots learning curves of MULTIPLE experiments, includes only validation accuracy \"\"\"\n",
    "    results_df = summarize_results(results_log, exclude_cols=['hyperparams', 'val_acc'])\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for index, row in results_df.iterrows():\n",
    "        plt.plot(pd.DataFrame.from_dict(row['results']).set_index('epoch')['val_acc'], label=row['num_epochs'])\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(title=plot_variable)    \n",
    "    \n",
    "def plot_single_learning_curve(results): \n",
    "    \"\"\" Plots learning curve of a SINGLE experiment, includes both train and validation accuracy \"\"\"\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    results_df = results_df.set_index('epoch')\n",
    "    results_df.plot()\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's train and evaluate a basic model with a set of arbitrary hyperparameters to make sure the code runs correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Accuracy: 50.51%, Validation Accuracy: 51.60%\n",
      "Epoch: 0.16, Train Accuracy: 59.45%, Validation Accuracy: 59.44%\n",
      "Epoch: 0.32, Train Accuracy: 68.08%, Validation Accuracy: 67.80%\n",
      "Epoch: 0.48, Train Accuracy: 72.19%, Validation Accuracy: 70.92%\n",
      "Epoch: 0.64, Train Accuracy: 75.73%, Validation Accuracy: 74.10%\n",
      "Epoch: 0.80, Train Accuracy: 78.48%, Validation Accuracy: 76.40%\n",
      "Epoch: 0.96, Train Accuracy: 80.61%, Validation Accuracy: 78.38%\n",
      "Epoch: 1.00, Train Accuracy: 80.30%, Validation Accuracy: 77.88%\n",
      "Epoch: 1.16, Train Accuracy: 82.28%, Validation Accuracy: 79.32%\n",
      "Epoch: 1.32, Train Accuracy: 83.41%, Validation Accuracy: 80.70%\n",
      "Epoch: 1.48, Train Accuracy: 84.28%, Validation Accuracy: 80.90%\n",
      "Epoch: 1.64, Train Accuracy: 85.05%, Validation Accuracy: 81.20%\n",
      "Epoch: 1.80, Train Accuracy: 85.64%, Validation Accuracy: 81.82%\n",
      "Epoch: 1.96, Train Accuracy: 86.33%, Validation Accuracy: 82.38%\n",
      "Epoch: 2.00, Train Accuracy: 86.22%, Validation Accuracy: 82.16%\n",
      "Epoch: 2.16, Train Accuracy: 86.95%, Validation Accuracy: 82.44%\n",
      "Epoch: 2.32, Train Accuracy: 87.47%, Validation Accuracy: 82.64%\n",
      "Epoch: 2.48, Train Accuracy: 87.81%, Validation Accuracy: 82.86%\n",
      "Epoch: 2.64, Train Accuracy: 88.30%, Validation Accuracy: 83.08%\n",
      "Epoch: 2.80, Train Accuracy: 88.52%, Validation Accuracy: 83.04%\n",
      "Epoch: 2.96, Train Accuracy: 88.95%, Validation Accuracy: 83.50%\n",
      "Epoch: 3.00, Train Accuracy: 89.06%, Validation Accuracy: 83.62%\n",
      "Epoch: 3.16, Train Accuracy: 89.30%, Validation Accuracy: 83.52%\n",
      "Epoch: 3.32, Train Accuracy: 89.58%, Validation Accuracy: 83.66%\n",
      "Epoch: 3.48, Train Accuracy: 89.58%, Validation Accuracy: 83.70%\n",
      "Epoch: 3.64, Train Accuracy: 90.27%, Validation Accuracy: 83.68%\n",
      "Epoch: 3.80, Train Accuracy: 90.56%, Validation Accuracy: 83.78%\n",
      "Epoch: 3.96, Train Accuracy: 90.66%, Validation Accuracy: 83.98%\n",
      "Epoch: 4.00, Train Accuracy: 90.72%, Validation Accuracy: 84.08%\n",
      "Epoch: 4.16, Train Accuracy: 90.95%, Validation Accuracy: 83.94%\n",
      "Epoch: 4.32, Train Accuracy: 91.11%, Validation Accuracy: 83.92%\n",
      "Epoch: 4.48, Train Accuracy: 91.28%, Validation Accuracy: 83.70%\n",
      "Epoch: 4.64, Train Accuracy: 91.44%, Validation Accuracy: 83.64%\n",
      "Epoch: 4.80, Train Accuracy: 91.51%, Validation Accuracy: 83.52%\n",
      "Epoch: 4.96, Train Accuracy: 91.77%, Validation Accuracy: 83.90%\n",
      "Epoch: 5.00, Train Accuracy: 92.00%, Validation Accuracy: 83.90%\n",
      "Epoch: 5.16, Train Accuracy: 92.06%, Validation Accuracy: 83.98%\n",
      "Epoch: 5.32, Train Accuracy: 92.17%, Validation Accuracy: 83.80%\n",
      "Epoch: 5.48, Train Accuracy: 92.44%, Validation Accuracy: 83.72%\n",
      "Epoch: 5.64, Train Accuracy: 92.54%, Validation Accuracy: 83.58%\n",
      "Epoch: 5.80, Train Accuracy: 92.70%, Validation Accuracy: 83.52%\n",
      "Epoch: 5.96, Train Accuracy: 92.94%, Validation Accuracy: 83.74%\n",
      "Epoch: 6.00, Train Accuracy: 92.84%, Validation Accuracy: 83.70%\n",
      "Epoch: 6.16, Train Accuracy: 93.02%, Validation Accuracy: 83.88%\n",
      "Epoch: 6.32, Train Accuracy: 93.14%, Validation Accuracy: 83.42%\n",
      "Epoch: 6.48, Train Accuracy: 93.33%, Validation Accuracy: 83.50%\n",
      "Epoch: 6.64, Train Accuracy: 93.42%, Validation Accuracy: 83.84%\n",
      "Epoch: 6.80, Train Accuracy: 93.69%, Validation Accuracy: 83.50%\n",
      "Epoch: 6.96, Train Accuracy: 93.80%, Validation Accuracy: 83.82%\n",
      "Epoch: 7.00, Train Accuracy: 93.86%, Validation Accuracy: 83.48%\n",
      "Epoch: 7.16, Train Accuracy: 93.45%, Validation Accuracy: 83.84%\n",
      "Epoch: 7.32, Train Accuracy: 94.08%, Validation Accuracy: 83.48%\n",
      "Epoch: 7.48, Train Accuracy: 94.20%, Validation Accuracy: 83.60%\n",
      "Epoch: 7.64, Train Accuracy: 94.31%, Validation Accuracy: 83.22%\n",
      "Epoch: 7.80, Train Accuracy: 94.31%, Validation Accuracy: 83.00%\n",
      "Epoch: 7.96, Train Accuracy: 94.50%, Validation Accuracy: 82.88%\n",
      "Epoch: 8.00, Train Accuracy: 94.56%, Validation Accuracy: 83.60%\n",
      "Epoch: 8.16, Train Accuracy: 94.57%, Validation Accuracy: 83.00%\n",
      "Epoch: 8.32, Train Accuracy: 94.81%, Validation Accuracy: 82.92%\n",
      "Epoch: 8.48, Train Accuracy: 94.77%, Validation Accuracy: 83.54%\n",
      "Epoch: 8.64, Train Accuracy: 94.93%, Validation Accuracy: 83.22%\n",
      "Epoch: 8.80, Train Accuracy: 95.05%, Validation Accuracy: 83.36%\n",
      "Epoch: 8.96, Train Accuracy: 95.12%, Validation Accuracy: 82.58%\n",
      "Epoch: 9.00, Train Accuracy: 95.24%, Validation Accuracy: 82.96%\n",
      "Epoch: 9.16, Train Accuracy: 95.34%, Validation Accuracy: 83.02%\n",
      "Epoch: 9.32, Train Accuracy: 95.34%, Validation Accuracy: 83.26%\n",
      "Epoch: 9.48, Train Accuracy: 95.48%, Validation Accuracy: 83.12%\n",
      "Epoch: 9.64, Train Accuracy: 95.53%, Validation Accuracy: 83.30%\n",
      "Epoch: 9.80, Train Accuracy: 95.69%, Validation Accuracy: 82.84%\n",
      "Epoch: 9.96, Train Accuracy: 95.72%, Validation Accuracy: 82.56%\n",
      "Epoch: 10.00, Train Accuracy: 95.33%, Validation Accuracy: 83.52%\n",
      "Experiment completed in 366 seconds with 84.08% validation accuracy.\n"
     ]
    }
   ],
   "source": [
    "results, hyperparams, runtime = run_experiment(\n",
    "    train_data_tokens, val_data_tokens, train_labels, val_labels, max_sentence_length=200, \n",
    "    max_vocab_size=10000, emb_dim=100, optim_algo='Adam', learning_rate=0.001, num_epochs=10, batch_size=32, \n",
    "    experiment_name='Test Basic Model', save_to_log=True, print_summary=True, print_intermediate=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to obtain pretty good results already: ~84% accuracy on unseen validation dataset. We will proceed to try different proprocessing and hyperparameters next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curve \n",
    "plot_single_learning_curve(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we converted our text data to tokens with limited preprocessing: the standard SpaCy tokenizer was applied on the lowercased text. In this section, we experiment with other tokenization schemes: \n",
    "\n",
    "0. Lowercased Text (\"Standard\") \n",
    "1. Lowercased Text + Removed Punctuation \n",
    "2. Lowercased Text + Removed Punctuation + Removed Stopwards (SpaCy)\n",
    "3. Lowercased Text + Removed Punctuation + Removed Stopwards (NLTK)\n",
    "4. Lowercased Text + Removed Punctuation + Removed Stopwards (SpaCy) + Lemmatize \n",
    "5. Lowercased Text + Removed Punctuation + Removed Stopwards (NLTK) + Lemmatize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation \n",
    "spacy_stop_words = tokenizer.Defaults.stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are testing two sets of stopwords, one from SpaCy and another from NLTK. SpaCy's considerably longer, and includes a number of words that might be important in n-grams, e.g. 'not', 'very'. NLTK's is shorter and appears to be more conservative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'would', 'first', 'latterly', 'under', 'before', 'why', 'through', 'has', 'onto', 'give', 'via', 'into', 'thereafter', 'they', 'wherein', 'nevertheless', 'becoming', 'i', 'nowhere', 'or', 'anything', 'really', 'whole', 'front', 'nobody', 'within', 'often', 'still', 'not', 'becomes', 'next', 'much', 'might', 'none', 'the', 'anywhere', 'with', 'former', 'may', 'its', 'another', 'their', 'while', 'noone', 'across', 'thereby', 'up', 'because', 'empty', 'than', 'ten', 'meanwhile', 'therein', 'three', 'whenever', 'whether', 'fifty', 'afterwards', 'had', 'used', 'nothing', 'although', 'per', 'amount', 'thus', 'at', 'over', 'besides', 'my', 'already', 'whence', 'against', 'thereupon', 'each', 'whoever', 'whom', 'hereby', 'name', 'seeming', 'thence', 'fifteen', 'him', 'only', 'some', 'could', 'which', 'call', 'hereafter', 'neither', 'somewhere', 'being', 'take', 'yours', 'between', 'more', 'five', 'too', 'perhaps', 'formerly', 'beforehand', 'doing', 'many', 'otherwise', 'part', 'and', 'now', 'these', 'please', 'bottom', 'rather', 'anyway', 'say', 'two', 'off', 'elsewhere', 'namely', 'about', 'ours', 'back', 'whereafter', 'but', 'was', 'wherever', 'everyone', 'twenty', 'those', 'he', 'various', 'twelve', 'amongst', 'once', 'thru', 'whatever', 'keep', 'them', 'without', 'after', 'latter', 'anyone', 'yet', 'anyhow', 'side', 'hers', 'always', 'in', 'me', 'unless', 'everything', 'are', 'done', 'here', 'full', 'such', 'other', 'been', 'itself', 'mostly', 'one', 'no', 'any', 'enough', 'beside', 'either', 'together', 'hereupon', 'well', 'it', 'whereupon', 'we', 'by', 'as', 'for', 'became', 'regarding', 'go', 'when', 'most', 'also', 'this', 'out', 'himself', 'made', 'nine', 'beyond', 'then', 'sometimes', 'last', 'to', 'others', 'sometime', 'behind', 'our', 'myself', 'that', 're', 'sixty', 'both', 'ca', 'something', 'herein', 'along', 'nor', 'down', 'third', 'alone', 'very', 'whither', 'themselves', 'seem', 'does', 'moreover', 'indeed', 'several', 'somehow', 'whereby', 'forty', 'ourselves', 'seems', 'everywhere', 'did', 'mine', 'make', 'again', 'move', 'see', 'she', 'since', 'can', 'throughout', 'every', 'of', 'your', 'own', 'further', 'how', 'on', 'ever', 'a', 'seemed', 'though', 'cannot', 'yourself', 'except', 'all', 'is', 'least', 'where', 'his', 'us', 'below', 'must', 'be', 'were', 'if', 'serious', 'therefore', 'using', 'an', 'never', 'should', 'among', 'yourselves', 'put', 'almost', 'same', 'else', 'from', 'get', 'what', 'top', 'her', 'however', 'eight', 'will', 'have', 'hence', 'four', 'six', 'just', 'even', 'become', 'until', 'show', 'someone', 'whose', 'herself', 'am', 'do', 'above', 'few', 'hundred', 'so', 'whereas', 'eleven', 'upon', 'you', 'less', 'there', 'who', 'around', 'quite', 'due', 'during', 'towards', 'toward'}\n"
     ]
    }
   ],
   "source": [
    "print(spacy_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define various tokenization functions and store as a dictionary \n",
    "\n",
    "def remove_punc(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def remove_punc_spacy_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in spacy_stop_words)]\n",
    "\n",
    "def remove_punc_nltk_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in nltk_stop_words)]\n",
    "\n",
    "def remove_punc_spacy_stopwords_lemmatize(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.lemma_.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.lemma_.lower() not in spacy_stop_words)]\n",
    "\n",
    "def remove_punc_nltk_stopwords_lemmatize(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.lemma_.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.lemma_.lower() not in nltk_stop_words)]\n",
    "\n",
    "tokenization_schemes = {'lowercase': lower_case, \n",
    "                        'lowercase_no_punc': remove_punc, \n",
    "                        'lowercase_no_punc_stopwords_spacy': remove_punc_spacy_stopwords, \n",
    "                        'lowercase_no_punc_stopwords_nltk': remove_punc_nltk_stopwords, \n",
    "                        'lowercase_no_punc_stopwords_spacy_lemmatize': remove_punc_spacy_stopwords_lemmatize, \n",
    "                        'lowercase_no_punc_stopwords_nltk_lemmatize': remove_punc_nltk_stopwords_lemmatize}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing with scheme: lowercase ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085d3b4b41434526ab832f5b72979ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/train_data_tokens.p in 0.7 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbdf5b769eb4990986c51eaf10fec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/val_data_tokens.p in 0.1 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e6d4de108f4266b993208fcb11b46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase/test_data_tokens.p in 0.7 minutes\n",
      "Tokenizing with scheme: lowercase_no_punc ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f2ffbdf11e4144b3078eeb44e5914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc/train_data_tokens.p in 0.6 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505f4b38de8e4d05b6894313711f453d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc/val_data_tokens.p in 0.1 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a9e0fb59554ba198e0fb9f96f2eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc/test_data_tokens.p in 0.7 minutes\n",
      "Tokenizing with scheme: lowercase_no_punc_stopwords_spacy ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc277b50aef4f929981c9e53ae9cfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy/train_data_tokens.p in 0.6 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4335d992697043df9686c04446625b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy/val_data_tokens.p in 0.1 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b23c4368d34637ba462bd8c0f65a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy/test_data_tokens.p in 0.7 minutes\n",
      "Tokenizing with scheme: lowercase_no_punc_stopwords_nltk ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb7e9c2e5654fb4b8ec344224fd028a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk/train_data_tokens.p in 0.7 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6a0ea88ef34180b638a0776b90de7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbfcc086bc64c009b7eda96b48bad20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk/test_data_tokens.p in 0.8 minutes\n",
      "Tokenizing with scheme: lowercase_no_punc_stopwords_spacy_lemmatize ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f4df7bdaed4ccd88f862bf20aaba09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy_lemmatize/train_data_tokens.p in 0.6 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb1853db0f94982bb1e1aa6ac9b9b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy_lemmatize/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a22602e59c43a3ba73c842e3413ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_spacy_lemmatize/test_data_tokens.p in 0.8 minutes\n",
      "Tokenizing with scheme: lowercase_no_punc_stopwords_nltk_lemmatize ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2462575e564282af9553094e8d0c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk_lemmatize/train_data_tokens.p in 0.8 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f9b8b677df41feab63519c3da10617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk_lemmatize/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f855017fce834613a3bb2471c2b0f477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/lowercase_no_punc_stopwords_nltk_lemmatize/test_data_tokens.p in 1.1 minutes\n"
     ]
    }
   ],
   "source": [
    "# run various tokenization experiments and save down as tokens \n",
    "\n",
    "for t_scheme in tokenization_schemes:     \n",
    "    print (\"Tokenizing with scheme: {} ...\".format(t_scheme))\n",
    "    _, _, _ = tokenize_datasets_to_disk(train_data, val_data, test_data, \n",
    "                                        processing_func=tokenization_schemes[t_scheme], folder_name=t_scheme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokenization experiments \n",
    "\n",
    "for t_scheme in tokenization_schemes: \n",
    "    \n",
    "    print (\"Training with and {} tokenization scheme...\".format(t_scheme))\n",
    "    \n",
    "    # load tokens from disk \n",
    "    train_data_tokens, val_data_tokens, test_data_tokens = load_tokens_from_disk(folder_name=t_scheme)\n",
    "    \n",
    "    # train and evaluate\n",
    "    results, hyperparams, runtime = run_experiment(\n",
    "        train_data_tokens, val_data_tokens, train_labels, val_labels, max_sentence_length=200, \n",
    "        max_vocab_size=10000, emb_dim=100, optim_algo='Adam', learning_rate=0.001, num_epochs=1, batch_size=32, \n",
    "        token_scheme=t_scheme, experiment_name='Tokenization_00', save_to_log=True, print_results=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with and lowercase tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 50.02%, Validation Accuracy: 50.04%\n",
      "Epoch: 0.16, Train Accuracy: 63.98%, Validation Accuracy: 62.34%\n",
      "Epoch: 0.32, Train Accuracy: 68.59%, Validation Accuracy: 66.26%\n",
      "Epoch: 0.48, Train Accuracy: 73.07%, Validation Accuracy: 71.28%\n",
      "Epoch: 0.64, Train Accuracy: 75.28%, Validation Accuracy: 73.36%\n",
      "Epoch: 0.80, Train Accuracy: 77.62%, Validation Accuracy: 75.00%\n",
      "Epoch: 0.96, Train Accuracy: 79.53%, Validation Accuracy: 76.54%\n",
      "Epoch: 1.00, Train Accuracy: 79.84%, Validation Accuracy: 77.06%\n",
      "Epoch: 1.16, Train Accuracy: 81.39%, Validation Accuracy: 78.32%\n",
      "Epoch: 1.32, Train Accuracy: 82.30%, Validation Accuracy: 78.94%\n",
      "Epoch: 1.48, Train Accuracy: 83.62%, Validation Accuracy: 79.84%\n",
      "Epoch: 1.64, Train Accuracy: 84.39%, Validation Accuracy: 80.96%\n",
      "Epoch: 1.80, Train Accuracy: 85.31%, Validation Accuracy: 81.04%\n",
      "Epoch: 1.96, Train Accuracy: 85.89%, Validation Accuracy: 81.96%\n",
      "Epoch: 2.00, Train Accuracy: 86.11%, Validation Accuracy: 82.06%\n",
      "Epoch: 2.16, Train Accuracy: 86.55%, Validation Accuracy: 82.22%\n",
      "Epoch: 2.32, Train Accuracy: 86.83%, Validation Accuracy: 82.90%\n",
      "Epoch: 2.48, Train Accuracy: 87.28%, Validation Accuracy: 83.22%\n",
      "Epoch: 2.64, Train Accuracy: 87.65%, Validation Accuracy: 83.48%\n",
      "Epoch: 2.80, Train Accuracy: 88.23%, Validation Accuracy: 83.52%\n",
      "Epoch: 2.96, Train Accuracy: 88.53%, Validation Accuracy: 83.88%\n",
      "Epoch: 3.00, Train Accuracy: 88.64%, Validation Accuracy: 83.76%\n",
      "Epoch: 3.16, Train Accuracy: 89.03%, Validation Accuracy: 83.74%\n",
      "Epoch: 3.32, Train Accuracy: 89.17%, Validation Accuracy: 83.86%\n",
      "Epoch: 3.48, Train Accuracy: 89.73%, Validation Accuracy: 83.88%\n",
      "Epoch: 3.64, Train Accuracy: 90.03%, Validation Accuracy: 84.00%\n",
      "Epoch: 3.80, Train Accuracy: 90.30%, Validation Accuracy: 84.02%\n",
      "Epoch: 3.96, Train Accuracy: 90.50%, Validation Accuracy: 84.10%\n",
      "Epoch: 4.00, Train Accuracy: 90.56%, Validation Accuracy: 84.26%\n",
      "Epoch: 4.16, Train Accuracy: 90.73%, Validation Accuracy: 84.28%\n",
      "Epoch: 4.32, Train Accuracy: 90.61%, Validation Accuracy: 84.32%\n",
      "Epoch: 4.48, Train Accuracy: 91.13%, Validation Accuracy: 84.40%\n",
      "Epoch: 4.64, Train Accuracy: 91.28%, Validation Accuracy: 84.40%\n",
      "Epoch: 4.80, Train Accuracy: 91.46%, Validation Accuracy: 84.40%\n",
      "Epoch: 4.96, Train Accuracy: 91.69%, Validation Accuracy: 84.70%\n",
      "Epoch: 5.00, Train Accuracy: 91.75%, Validation Accuracy: 84.20%\n",
      "Epoch: 5.16, Train Accuracy: 91.87%, Validation Accuracy: 84.44%\n",
      "Epoch: 5.32, Train Accuracy: 92.10%, Validation Accuracy: 84.32%\n",
      "Epoch: 5.48, Train Accuracy: 92.27%, Validation Accuracy: 84.32%\n",
      "Epoch: 5.64, Train Accuracy: 92.40%, Validation Accuracy: 84.48%\n",
      "Epoch: 5.80, Train Accuracy: 92.42%, Validation Accuracy: 84.52%\n",
      "Epoch: 5.96, Train Accuracy: 92.67%, Validation Accuracy: 84.42%\n",
      "Epoch: 6.00, Train Accuracy: 92.53%, Validation Accuracy: 83.60%\n",
      "Epoch: 6.16, Train Accuracy: 92.83%, Validation Accuracy: 84.60%\n",
      "Epoch: 6.32, Train Accuracy: 93.00%, Validation Accuracy: 84.48%\n",
      "Epoch: 6.48, Train Accuracy: 93.17%, Validation Accuracy: 84.42%\n",
      "Epoch: 6.64, Train Accuracy: 93.36%, Validation Accuracy: 84.28%\n",
      "Epoch: 6.80, Train Accuracy: 93.49%, Validation Accuracy: 84.36%\n",
      "Epoch: 6.96, Train Accuracy: 93.59%, Validation Accuracy: 84.44%\n",
      "Epoch: 7.00, Train Accuracy: 93.43%, Validation Accuracy: 83.64%\n",
      "Epoch: 7.16, Train Accuracy: 93.72%, Validation Accuracy: 83.94%\n",
      "Epoch: 7.32, Train Accuracy: 93.91%, Validation Accuracy: 84.00%\n",
      "Epoch: 7.48, Train Accuracy: 94.04%, Validation Accuracy: 84.12%\n",
      "Epoch: 7.64, Train Accuracy: 93.98%, Validation Accuracy: 83.84%\n",
      "Epoch: 7.80, Train Accuracy: 94.34%, Validation Accuracy: 84.08%\n",
      "Epoch: 7.96, Train Accuracy: 94.33%, Validation Accuracy: 83.42%\n",
      "Epoch: 8.00, Train Accuracy: 94.48%, Validation Accuracy: 83.88%\n",
      "Epoch: 8.16, Train Accuracy: 94.56%, Validation Accuracy: 83.72%\n",
      "Epoch: 8.32, Train Accuracy: 94.69%, Validation Accuracy: 83.62%\n",
      "Epoch: 8.48, Train Accuracy: 94.64%, Validation Accuracy: 83.48%\n",
      "Epoch: 8.64, Train Accuracy: 94.83%, Validation Accuracy: 83.58%\n",
      "Epoch: 8.80, Train Accuracy: 94.97%, Validation Accuracy: 83.60%\n",
      "Epoch: 8.96, Train Accuracy: 95.10%, Validation Accuracy: 83.82%\n",
      "Epoch: 9.00, Train Accuracy: 95.02%, Validation Accuracy: 83.38%\n",
      "Epoch: 9.16, Train Accuracy: 95.19%, Validation Accuracy: 83.68%\n",
      "Epoch: 9.32, Train Accuracy: 95.32%, Validation Accuracy: 83.54%\n",
      "Epoch: 9.48, Train Accuracy: 95.50%, Validation Accuracy: 83.72%\n",
      "Epoch: 9.64, Train Accuracy: 95.51%, Validation Accuracy: 83.62%\n",
      "Epoch: 9.80, Train Accuracy: 95.56%, Validation Accuracy: 83.38%\n",
      "Epoch: 9.96, Train Accuracy: 95.62%, Validation Accuracy: 83.36%\n",
      "Epoch: 10.00, Train Accuracy: 95.64%, Validation Accuracy: 83.06%\n",
      "Training with and lowercase_no_punc tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 48.35%, Validation Accuracy: 49.24%\n",
      "Epoch: 0.16, Train Accuracy: 63.74%, Validation Accuracy: 63.56%\n",
      "Epoch: 0.32, Train Accuracy: 68.73%, Validation Accuracy: 68.32%\n",
      "Epoch: 0.48, Train Accuracy: 73.03%, Validation Accuracy: 71.18%\n",
      "Epoch: 0.64, Train Accuracy: 75.98%, Validation Accuracy: 73.52%\n",
      "Epoch: 0.80, Train Accuracy: 78.36%, Validation Accuracy: 75.46%\n",
      "Epoch: 0.96, Train Accuracy: 80.17%, Validation Accuracy: 77.52%\n",
      "Epoch: 1.00, Train Accuracy: 81.04%, Validation Accuracy: 78.06%\n",
      "Epoch: 1.16, Train Accuracy: 82.52%, Validation Accuracy: 79.24%\n",
      "Epoch: 1.32, Train Accuracy: 83.84%, Validation Accuracy: 80.42%\n",
      "Epoch: 1.48, Train Accuracy: 84.64%, Validation Accuracy: 80.76%\n",
      "Epoch: 1.64, Train Accuracy: 85.25%, Validation Accuracy: 81.82%\n",
      "Epoch: 1.80, Train Accuracy: 85.83%, Validation Accuracy: 81.74%\n",
      "Epoch: 1.96, Train Accuracy: 86.64%, Validation Accuracy: 82.58%\n",
      "Epoch: 2.00, Train Accuracy: 86.75%, Validation Accuracy: 82.82%\n",
      "Epoch: 2.16, Train Accuracy: 87.03%, Validation Accuracy: 82.58%\n",
      "Epoch: 2.32, Train Accuracy: 87.78%, Validation Accuracy: 83.04%\n",
      "Epoch: 2.48, Train Accuracy: 88.25%, Validation Accuracy: 83.36%\n",
      "Epoch: 2.64, Train Accuracy: 88.62%, Validation Accuracy: 83.48%\n",
      "Epoch: 2.80, Train Accuracy: 89.08%, Validation Accuracy: 83.74%\n",
      "Epoch: 2.96, Train Accuracy: 88.86%, Validation Accuracy: 83.72%\n",
      "Epoch: 3.00, Train Accuracy: 89.22%, Validation Accuracy: 83.18%\n",
      "Epoch: 3.16, Train Accuracy: 89.72%, Validation Accuracy: 84.20%\n",
      "Epoch: 3.32, Train Accuracy: 89.87%, Validation Accuracy: 83.52%\n",
      "Epoch: 3.48, Train Accuracy: 90.34%, Validation Accuracy: 84.58%\n",
      "Epoch: 3.64, Train Accuracy: 90.61%, Validation Accuracy: 84.54%\n",
      "Epoch: 3.80, Train Accuracy: 90.83%, Validation Accuracy: 84.60%\n",
      "Epoch: 3.96, Train Accuracy: 91.05%, Validation Accuracy: 84.62%\n",
      "Epoch: 4.00, Train Accuracy: 90.68%, Validation Accuracy: 84.56%\n",
      "Epoch: 4.16, Train Accuracy: 90.94%, Validation Accuracy: 84.44%\n",
      "Epoch: 4.32, Train Accuracy: 91.58%, Validation Accuracy: 84.56%\n",
      "Epoch: 4.48, Train Accuracy: 91.64%, Validation Accuracy: 85.14%\n",
      "Epoch: 4.64, Train Accuracy: 91.81%, Validation Accuracy: 84.88%\n",
      "Epoch: 4.80, Train Accuracy: 91.98%, Validation Accuracy: 85.04%\n",
      "Epoch: 4.96, Train Accuracy: 92.30%, Validation Accuracy: 85.06%\n",
      "Epoch: 5.00, Train Accuracy: 92.26%, Validation Accuracy: 84.88%\n",
      "Epoch: 5.16, Train Accuracy: 92.57%, Validation Accuracy: 84.74%\n",
      "Epoch: 5.32, Train Accuracy: 92.77%, Validation Accuracy: 84.84%\n",
      "Epoch: 5.48, Train Accuracy: 92.83%, Validation Accuracy: 85.00%\n",
      "Epoch: 5.64, Train Accuracy: 93.00%, Validation Accuracy: 85.00%\n",
      "Epoch: 5.80, Train Accuracy: 93.28%, Validation Accuracy: 84.86%\n",
      "Epoch: 5.96, Train Accuracy: 93.39%, Validation Accuracy: 84.88%\n",
      "Epoch: 6.00, Train Accuracy: 93.44%, Validation Accuracy: 85.02%\n",
      "Epoch: 6.16, Train Accuracy: 93.47%, Validation Accuracy: 84.70%\n",
      "Epoch: 6.32, Train Accuracy: 93.64%, Validation Accuracy: 85.08%\n",
      "Epoch: 6.48, Train Accuracy: 93.58%, Validation Accuracy: 84.84%\n",
      "Epoch: 6.64, Train Accuracy: 93.97%, Validation Accuracy: 84.80%\n",
      "Epoch: 6.80, Train Accuracy: 94.03%, Validation Accuracy: 84.74%\n",
      "Epoch: 6.96, Train Accuracy: 94.25%, Validation Accuracy: 84.82%\n",
      "Epoch: 7.00, Train Accuracy: 94.28%, Validation Accuracy: 85.02%\n",
      "Epoch: 7.16, Train Accuracy: 94.33%, Validation Accuracy: 85.06%\n",
      "Epoch: 7.32, Train Accuracy: 94.39%, Validation Accuracy: 84.84%\n",
      "Epoch: 7.48, Train Accuracy: 94.61%, Validation Accuracy: 84.92%\n",
      "Epoch: 7.64, Train Accuracy: 94.50%, Validation Accuracy: 84.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7.80, Train Accuracy: 94.42%, Validation Accuracy: 84.46%\n",
      "Epoch: 7.96, Train Accuracy: 94.68%, Validation Accuracy: 83.92%\n",
      "Epoch: 8.00, Train Accuracy: 94.88%, Validation Accuracy: 84.16%\n",
      "Epoch: 8.16, Train Accuracy: 94.94%, Validation Accuracy: 84.62%\n",
      "Epoch: 8.32, Train Accuracy: 95.08%, Validation Accuracy: 84.46%\n",
      "Epoch: 8.48, Train Accuracy: 95.08%, Validation Accuracy: 84.08%\n",
      "Epoch: 8.64, Train Accuracy: 95.34%, Validation Accuracy: 84.52%\n",
      "Epoch: 8.80, Train Accuracy: 95.42%, Validation Accuracy: 84.48%\n",
      "Epoch: 8.96, Train Accuracy: 95.56%, Validation Accuracy: 84.42%\n",
      "Epoch: 9.00, Train Accuracy: 95.61%, Validation Accuracy: 84.56%\n",
      "Epoch: 9.16, Train Accuracy: 95.69%, Validation Accuracy: 84.36%\n",
      "Epoch: 9.32, Train Accuracy: 95.78%, Validation Accuracy: 84.24%\n",
      "Epoch: 9.48, Train Accuracy: 95.84%, Validation Accuracy: 84.22%\n",
      "Epoch: 9.64, Train Accuracy: 95.98%, Validation Accuracy: 83.78%\n",
      "Epoch: 9.80, Train Accuracy: 96.08%, Validation Accuracy: 83.88%\n",
      "Epoch: 9.96, Train Accuracy: 96.17%, Validation Accuracy: 83.96%\n",
      "Epoch: 10.00, Train Accuracy: 96.09%, Validation Accuracy: 83.92%\n",
      "Training with and lowercase_no_punc_stopwords_spacy tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 48.38%, Validation Accuracy: 49.38%\n",
      "Epoch: 0.16, Train Accuracy: 65.97%, Validation Accuracy: 65.84%\n",
      "Epoch: 0.32, Train Accuracy: 73.31%, Validation Accuracy: 71.62%\n",
      "Epoch: 0.48, Train Accuracy: 78.31%, Validation Accuracy: 76.04%\n",
      "Epoch: 0.64, Train Accuracy: 81.61%, Validation Accuracy: 78.72%\n",
      "Epoch: 0.80, Train Accuracy: 83.20%, Validation Accuracy: 80.48%\n",
      "Epoch: 0.96, Train Accuracy: 85.45%, Validation Accuracy: 81.54%\n",
      "Epoch: 1.00, Train Accuracy: 85.69%, Validation Accuracy: 81.64%\n",
      "Epoch: 1.16, Train Accuracy: 86.81%, Validation Accuracy: 82.84%\n",
      "Epoch: 1.32, Train Accuracy: 87.53%, Validation Accuracy: 82.92%\n",
      "Epoch: 1.48, Train Accuracy: 88.21%, Validation Accuracy: 83.62%\n",
      "Epoch: 1.64, Train Accuracy: 88.89%, Validation Accuracy: 84.32%\n",
      "Epoch: 1.80, Train Accuracy: 89.48%, Validation Accuracy: 84.56%\n",
      "Epoch: 1.96, Train Accuracy: 89.88%, Validation Accuracy: 84.94%\n",
      "Epoch: 2.00, Train Accuracy: 89.91%, Validation Accuracy: 84.78%\n",
      "Epoch: 2.16, Train Accuracy: 90.31%, Validation Accuracy: 85.24%\n",
      "Epoch: 2.32, Train Accuracy: 90.67%, Validation Accuracy: 85.28%\n",
      "Epoch: 2.48, Train Accuracy: 90.99%, Validation Accuracy: 85.56%\n",
      "Epoch: 2.64, Train Accuracy: 91.41%, Validation Accuracy: 85.50%\n",
      "Epoch: 2.80, Train Accuracy: 91.78%, Validation Accuracy: 85.92%\n",
      "Epoch: 2.96, Train Accuracy: 92.03%, Validation Accuracy: 86.26%\n",
      "Epoch: 3.00, Train Accuracy: 92.09%, Validation Accuracy: 86.42%\n",
      "Epoch: 3.16, Train Accuracy: 92.36%, Validation Accuracy: 86.26%\n",
      "Epoch: 3.32, Train Accuracy: 92.73%, Validation Accuracy: 86.48%\n",
      "Epoch: 3.48, Train Accuracy: 92.97%, Validation Accuracy: 86.38%\n",
      "Epoch: 3.64, Train Accuracy: 92.83%, Validation Accuracy: 85.64%\n",
      "Epoch: 3.80, Train Accuracy: 93.32%, Validation Accuracy: 86.02%\n",
      "Epoch: 3.96, Train Accuracy: 93.58%, Validation Accuracy: 85.80%\n",
      "Epoch: 4.00, Train Accuracy: 93.64%, Validation Accuracy: 85.98%\n",
      "Epoch: 4.16, Train Accuracy: 93.86%, Validation Accuracy: 86.18%\n",
      "Epoch: 4.32, Train Accuracy: 93.93%, Validation Accuracy: 85.96%\n",
      "Epoch: 4.48, Train Accuracy: 94.19%, Validation Accuracy: 86.12%\n",
      "Epoch: 4.64, Train Accuracy: 94.42%, Validation Accuracy: 86.24%\n",
      "Epoch: 4.80, Train Accuracy: 94.54%, Validation Accuracy: 86.00%\n",
      "Epoch: 4.96, Train Accuracy: 94.75%, Validation Accuracy: 85.48%\n",
      "Epoch: 5.00, Train Accuracy: 94.81%, Validation Accuracy: 85.76%\n",
      "Epoch: 5.16, Train Accuracy: 95.03%, Validation Accuracy: 85.88%\n",
      "Epoch: 5.32, Train Accuracy: 95.16%, Validation Accuracy: 85.84%\n",
      "Epoch: 5.48, Train Accuracy: 95.23%, Validation Accuracy: 85.74%\n",
      "Epoch: 5.64, Train Accuracy: 95.47%, Validation Accuracy: 85.42%\n",
      "Epoch: 5.80, Train Accuracy: 95.60%, Validation Accuracy: 85.68%\n",
      "Epoch: 5.96, Train Accuracy: 95.61%, Validation Accuracy: 85.00%\n",
      "Epoch: 6.00, Train Accuracy: 95.81%, Validation Accuracy: 85.30%\n",
      "Epoch: 6.16, Train Accuracy: 95.81%, Validation Accuracy: 85.88%\n",
      "Epoch: 6.32, Train Accuracy: 96.00%, Validation Accuracy: 85.46%\n",
      "Epoch: 6.48, Train Accuracy: 95.95%, Validation Accuracy: 85.72%\n",
      "Epoch: 6.64, Train Accuracy: 96.31%, Validation Accuracy: 85.18%\n",
      "Epoch: 6.80, Train Accuracy: 96.46%, Validation Accuracy: 84.82%\n",
      "Epoch: 6.96, Train Accuracy: 96.45%, Validation Accuracy: 84.98%\n",
      "Epoch: 7.00, Train Accuracy: 96.52%, Validation Accuracy: 85.46%\n",
      "Epoch: 7.16, Train Accuracy: 96.56%, Validation Accuracy: 85.38%\n",
      "Epoch: 7.32, Train Accuracy: 96.66%, Validation Accuracy: 85.32%\n",
      "Epoch: 7.48, Train Accuracy: 96.89%, Validation Accuracy: 84.98%\n",
      "Epoch: 7.64, Train Accuracy: 96.92%, Validation Accuracy: 84.94%\n",
      "Epoch: 7.80, Train Accuracy: 97.08%, Validation Accuracy: 84.84%\n",
      "Epoch: 7.96, Train Accuracy: 97.19%, Validation Accuracy: 84.76%\n",
      "Epoch: 8.00, Train Accuracy: 97.22%, Validation Accuracy: 84.76%\n",
      "Epoch: 8.16, Train Accuracy: 97.28%, Validation Accuracy: 84.92%\n",
      "Epoch: 8.32, Train Accuracy: 97.33%, Validation Accuracy: 84.78%\n",
      "Epoch: 8.48, Train Accuracy: 97.36%, Validation Accuracy: 84.64%\n",
      "Epoch: 8.64, Train Accuracy: 97.48%, Validation Accuracy: 84.40%\n",
      "Epoch: 8.80, Train Accuracy: 97.67%, Validation Accuracy: 84.58%\n",
      "Epoch: 8.96, Train Accuracy: 97.78%, Validation Accuracy: 84.50%\n",
      "Epoch: 9.00, Train Accuracy: 97.81%, Validation Accuracy: 84.58%\n",
      "Epoch: 9.16, Train Accuracy: 97.83%, Validation Accuracy: 84.46%\n",
      "Epoch: 9.32, Train Accuracy: 97.94%, Validation Accuracy: 84.40%\n",
      "Epoch: 9.48, Train Accuracy: 97.86%, Validation Accuracy: 84.20%\n",
      "Epoch: 9.64, Train Accuracy: 98.04%, Validation Accuracy: 84.34%\n",
      "Epoch: 9.80, Train Accuracy: 98.10%, Validation Accuracy: 84.20%\n",
      "Epoch: 9.96, Train Accuracy: 98.23%, Validation Accuracy: 84.16%\n",
      "Epoch: 10.00, Train Accuracy: 98.28%, Validation Accuracy: 84.08%\n",
      "Training with and lowercase_no_punc_stopwords_nltk tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 49.24%, Validation Accuracy: 50.14%\n",
      "Epoch: 0.16, Train Accuracy: 61.43%, Validation Accuracy: 59.34%\n",
      "Epoch: 0.32, Train Accuracy: 70.97%, Validation Accuracy: 69.10%\n",
      "Epoch: 0.48, Train Accuracy: 76.38%, Validation Accuracy: 73.60%\n",
      "Epoch: 0.64, Train Accuracy: 77.92%, Validation Accuracy: 75.40%\n",
      "Epoch: 0.80, Train Accuracy: 83.05%, Validation Accuracy: 79.70%\n",
      "Epoch: 0.96, Train Accuracy: 85.06%, Validation Accuracy: 81.82%\n",
      "Epoch: 1.00, Train Accuracy: 85.28%, Validation Accuracy: 82.48%\n",
      "Epoch: 1.16, Train Accuracy: 86.42%, Validation Accuracy: 83.54%\n",
      "Epoch: 1.32, Train Accuracy: 87.50%, Validation Accuracy: 84.32%\n",
      "Epoch: 1.48, Train Accuracy: 88.02%, Validation Accuracy: 83.74%\n",
      "Epoch: 1.64, Train Accuracy: 88.87%, Validation Accuracy: 84.92%\n",
      "Epoch: 1.80, Train Accuracy: 89.52%, Validation Accuracy: 85.04%\n",
      "Epoch: 1.96, Train Accuracy: 89.78%, Validation Accuracy: 85.46%\n",
      "Epoch: 2.00, Train Accuracy: 89.81%, Validation Accuracy: 85.60%\n",
      "Epoch: 2.16, Train Accuracy: 90.22%, Validation Accuracy: 85.76%\n",
      "Epoch: 2.32, Train Accuracy: 90.62%, Validation Accuracy: 85.74%\n",
      "Epoch: 2.48, Train Accuracy: 90.97%, Validation Accuracy: 85.96%\n",
      "Epoch: 2.64, Train Accuracy: 91.14%, Validation Accuracy: 85.56%\n",
      "Epoch: 2.80, Train Accuracy: 91.59%, Validation Accuracy: 86.20%\n",
      "Epoch: 2.96, Train Accuracy: 91.82%, Validation Accuracy: 85.94%\n",
      "Epoch: 3.00, Train Accuracy: 91.94%, Validation Accuracy: 86.36%\n",
      "Epoch: 3.16, Train Accuracy: 92.27%, Validation Accuracy: 86.44%\n",
      "Epoch: 3.32, Train Accuracy: 92.53%, Validation Accuracy: 86.16%\n",
      "Epoch: 3.48, Train Accuracy: 92.57%, Validation Accuracy: 85.64%\n",
      "Epoch: 3.64, Train Accuracy: 93.02%, Validation Accuracy: 86.42%\n",
      "Epoch: 3.80, Train Accuracy: 93.11%, Validation Accuracy: 86.32%\n",
      "Epoch: 3.96, Train Accuracy: 93.31%, Validation Accuracy: 85.94%\n",
      "Epoch: 4.00, Train Accuracy: 93.39%, Validation Accuracy: 86.68%\n",
      "Epoch: 4.16, Train Accuracy: 93.33%, Validation Accuracy: 86.64%\n",
      "Epoch: 4.32, Train Accuracy: 93.69%, Validation Accuracy: 86.08%\n",
      "Epoch: 4.48, Train Accuracy: 93.89%, Validation Accuracy: 86.30%\n",
      "Epoch: 4.64, Train Accuracy: 94.08%, Validation Accuracy: 86.16%\n",
      "Epoch: 4.80, Train Accuracy: 94.30%, Validation Accuracy: 86.42%\n",
      "Epoch: 4.96, Train Accuracy: 94.47%, Validation Accuracy: 86.42%\n",
      "Epoch: 5.00, Train Accuracy: 94.56%, Validation Accuracy: 86.32%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5.16, Train Accuracy: 94.82%, Validation Accuracy: 86.04%\n",
      "Epoch: 5.32, Train Accuracy: 94.88%, Validation Accuracy: 86.16%\n",
      "Epoch: 5.48, Train Accuracy: 95.14%, Validation Accuracy: 86.20%\n",
      "Epoch: 5.64, Train Accuracy: 95.23%, Validation Accuracy: 86.24%\n",
      "Epoch: 5.80, Train Accuracy: 95.34%, Validation Accuracy: 86.18%\n",
      "Epoch: 5.96, Train Accuracy: 95.50%, Validation Accuracy: 85.86%\n",
      "Epoch: 6.00, Train Accuracy: 95.45%, Validation Accuracy: 86.20%\n",
      "Epoch: 6.16, Train Accuracy: 95.61%, Validation Accuracy: 86.30%\n",
      "Epoch: 6.32, Train Accuracy: 95.69%, Validation Accuracy: 86.06%\n",
      "Epoch: 6.48, Train Accuracy: 95.77%, Validation Accuracy: 85.72%\n",
      "Epoch: 6.64, Train Accuracy: 95.98%, Validation Accuracy: 85.82%\n",
      "Epoch: 6.80, Train Accuracy: 96.00%, Validation Accuracy: 85.40%\n",
      "Epoch: 6.96, Train Accuracy: 96.27%, Validation Accuracy: 85.86%\n",
      "Epoch: 7.00, Train Accuracy: 96.25%, Validation Accuracy: 85.02%\n",
      "Epoch: 7.16, Train Accuracy: 96.44%, Validation Accuracy: 85.44%\n",
      "Epoch: 7.32, Train Accuracy: 96.45%, Validation Accuracy: 85.78%\n",
      "Epoch: 7.48, Train Accuracy: 96.59%, Validation Accuracy: 85.20%\n",
      "Epoch: 7.64, Train Accuracy: 96.63%, Validation Accuracy: 85.30%\n",
      "Epoch: 7.80, Train Accuracy: 96.79%, Validation Accuracy: 85.90%\n",
      "Epoch: 7.96, Train Accuracy: 97.03%, Validation Accuracy: 85.32%\n",
      "Epoch: 8.00, Train Accuracy: 97.06%, Validation Accuracy: 85.08%\n",
      "Epoch: 8.16, Train Accuracy: 97.00%, Validation Accuracy: 85.04%\n",
      "Epoch: 8.32, Train Accuracy: 97.11%, Validation Accuracy: 85.42%\n",
      "Epoch: 8.48, Train Accuracy: 97.27%, Validation Accuracy: 85.12%\n",
      "Epoch: 8.64, Train Accuracy: 97.38%, Validation Accuracy: 85.02%\n",
      "Epoch: 8.80, Train Accuracy: 97.37%, Validation Accuracy: 84.90%\n",
      "Epoch: 8.96, Train Accuracy: 97.52%, Validation Accuracy: 85.24%\n",
      "Epoch: 9.00, Train Accuracy: 97.58%, Validation Accuracy: 85.14%\n",
      "Epoch: 9.16, Train Accuracy: 97.58%, Validation Accuracy: 84.90%\n",
      "Epoch: 9.32, Train Accuracy: 97.63%, Validation Accuracy: 84.96%\n",
      "Epoch: 9.48, Train Accuracy: 97.62%, Validation Accuracy: 84.68%\n",
      "Epoch: 9.64, Train Accuracy: 97.81%, Validation Accuracy: 84.84%\n",
      "Epoch: 9.80, Train Accuracy: 97.85%, Validation Accuracy: 84.32%\n",
      "Epoch: 9.96, Train Accuracy: 98.00%, Validation Accuracy: 85.08%\n",
      "Epoch: 10.00, Train Accuracy: 98.03%, Validation Accuracy: 85.08%\n",
      "Training with and lowercase_no_punc_stopwords_spacy_lemmatize tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 50.10%, Validation Accuracy: 50.00%\n",
      "Epoch: 0.16, Train Accuracy: 63.83%, Validation Accuracy: 64.24%\n",
      "Epoch: 0.32, Train Accuracy: 72.50%, Validation Accuracy: 71.26%\n",
      "Epoch: 0.48, Train Accuracy: 76.45%, Validation Accuracy: 75.22%\n",
      "Epoch: 0.64, Train Accuracy: 80.48%, Validation Accuracy: 78.30%\n",
      "Epoch: 0.80, Train Accuracy: 82.98%, Validation Accuracy: 80.46%\n",
      "Epoch: 0.96, Train Accuracy: 84.72%, Validation Accuracy: 81.66%\n",
      "Epoch: 1.00, Train Accuracy: 84.83%, Validation Accuracy: 82.36%\n",
      "Epoch: 1.16, Train Accuracy: 85.97%, Validation Accuracy: 83.22%\n",
      "Epoch: 1.32, Train Accuracy: 86.91%, Validation Accuracy: 83.32%\n",
      "Epoch: 1.48, Train Accuracy: 87.83%, Validation Accuracy: 83.62%\n",
      "Epoch: 1.64, Train Accuracy: 88.45%, Validation Accuracy: 84.04%\n",
      "Epoch: 1.80, Train Accuracy: 89.00%, Validation Accuracy: 84.50%\n",
      "Epoch: 1.96, Train Accuracy: 89.36%, Validation Accuracy: 84.24%\n",
      "Epoch: 2.00, Train Accuracy: 89.67%, Validation Accuracy: 84.94%\n",
      "Epoch: 2.16, Train Accuracy: 90.03%, Validation Accuracy: 84.68%\n",
      "Epoch: 2.32, Train Accuracy: 90.39%, Validation Accuracy: 84.82%\n",
      "Epoch: 2.48, Train Accuracy: 90.73%, Validation Accuracy: 84.88%\n",
      "Epoch: 2.64, Train Accuracy: 91.09%, Validation Accuracy: 85.12%\n",
      "Epoch: 2.80, Train Accuracy: 91.45%, Validation Accuracy: 85.02%\n",
      "Epoch: 2.96, Train Accuracy: 91.69%, Validation Accuracy: 85.14%\n",
      "Epoch: 3.00, Train Accuracy: 91.75%, Validation Accuracy: 85.04%\n",
      "Epoch: 3.16, Train Accuracy: 92.01%, Validation Accuracy: 85.14%\n",
      "Epoch: 3.32, Train Accuracy: 92.28%, Validation Accuracy: 85.22%\n",
      "Epoch: 3.48, Train Accuracy: 92.55%, Validation Accuracy: 85.08%\n",
      "Epoch: 3.64, Train Accuracy: 92.69%, Validation Accuracy: 85.20%\n",
      "Epoch: 3.80, Train Accuracy: 92.99%, Validation Accuracy: 85.06%\n",
      "Epoch: 3.96, Train Accuracy: 93.27%, Validation Accuracy: 84.96%\n",
      "Epoch: 4.00, Train Accuracy: 93.31%, Validation Accuracy: 84.96%\n",
      "Epoch: 4.16, Train Accuracy: 93.48%, Validation Accuracy: 84.78%\n",
      "Epoch: 4.32, Train Accuracy: 93.61%, Validation Accuracy: 84.52%\n",
      "Epoch: 4.48, Train Accuracy: 93.86%, Validation Accuracy: 84.58%\n",
      "Epoch: 4.64, Train Accuracy: 93.89%, Validation Accuracy: 84.48%\n",
      "Epoch: 4.80, Train Accuracy: 94.03%, Validation Accuracy: 84.36%\n",
      "Epoch: 4.96, Train Accuracy: 94.39%, Validation Accuracy: 85.00%\n",
      "Epoch: 5.00, Train Accuracy: 94.32%, Validation Accuracy: 85.00%\n",
      "Epoch: 5.16, Train Accuracy: 94.54%, Validation Accuracy: 84.66%\n",
      "Epoch: 5.32, Train Accuracy: 94.67%, Validation Accuracy: 84.66%\n",
      "Epoch: 5.48, Train Accuracy: 94.77%, Validation Accuracy: 84.68%\n",
      "Epoch: 5.64, Train Accuracy: 94.89%, Validation Accuracy: 84.70%\n",
      "Epoch: 5.80, Train Accuracy: 95.05%, Validation Accuracy: 84.90%\n",
      "Epoch: 5.96, Train Accuracy: 95.23%, Validation Accuracy: 84.22%\n",
      "Epoch: 6.00, Train Accuracy: 95.24%, Validation Accuracy: 84.12%\n",
      "Epoch: 6.16, Train Accuracy: 95.39%, Validation Accuracy: 84.52%\n",
      "Epoch: 6.32, Train Accuracy: 95.52%, Validation Accuracy: 84.00%\n",
      "Epoch: 6.48, Train Accuracy: 95.69%, Validation Accuracy: 84.34%\n",
      "Epoch: 6.64, Train Accuracy: 95.75%, Validation Accuracy: 84.68%\n",
      "Epoch: 6.80, Train Accuracy: 95.86%, Validation Accuracy: 84.26%\n",
      "Epoch: 6.96, Train Accuracy: 96.02%, Validation Accuracy: 84.74%\n",
      "Epoch: 7.00, Train Accuracy: 96.08%, Validation Accuracy: 84.10%\n",
      "Epoch: 7.16, Train Accuracy: 96.17%, Validation Accuracy: 84.06%\n",
      "Epoch: 7.32, Train Accuracy: 96.33%, Validation Accuracy: 83.66%\n",
      "Epoch: 7.48, Train Accuracy: 96.31%, Validation Accuracy: 84.16%\n",
      "Epoch: 7.64, Train Accuracy: 96.53%, Validation Accuracy: 83.90%\n",
      "Epoch: 7.80, Train Accuracy: 96.64%, Validation Accuracy: 84.08%\n",
      "Epoch: 7.96, Train Accuracy: 96.76%, Validation Accuracy: 84.24%\n",
      "Epoch: 8.00, Train Accuracy: 96.71%, Validation Accuracy: 83.74%\n",
      "Epoch: 8.16, Train Accuracy: 96.82%, Validation Accuracy: 83.80%\n",
      "Epoch: 8.32, Train Accuracy: 96.95%, Validation Accuracy: 83.92%\n",
      "Epoch: 8.48, Train Accuracy: 96.95%, Validation Accuracy: 83.42%\n",
      "Epoch: 8.64, Train Accuracy: 97.12%, Validation Accuracy: 83.48%\n",
      "Epoch: 8.80, Train Accuracy: 97.04%, Validation Accuracy: 82.68%\n",
      "Epoch: 8.96, Train Accuracy: 97.34%, Validation Accuracy: 83.44%\n",
      "Epoch: 9.00, Train Accuracy: 97.44%, Validation Accuracy: 83.42%\n",
      "Epoch: 9.16, Train Accuracy: 97.48%, Validation Accuracy: 83.38%\n",
      "Epoch: 9.32, Train Accuracy: 97.52%, Validation Accuracy: 83.26%\n",
      "Epoch: 9.48, Train Accuracy: 97.45%, Validation Accuracy: 83.82%\n",
      "Epoch: 9.64, Train Accuracy: 97.60%, Validation Accuracy: 82.88%\n",
      "Epoch: 9.80, Train Accuracy: 97.67%, Validation Accuracy: 83.20%\n",
      "Epoch: 9.96, Train Accuracy: 97.89%, Validation Accuracy: 82.92%\n",
      "Epoch: 10.00, Train Accuracy: 97.90%, Validation Accuracy: 82.90%\n",
      "Training with and lowercase_no_punc_stopwords_nltk_lemmatize tokenization scheme...\n",
      "Epoch: 0.00, Train Accuracy: 44.44%, Validation Accuracy: 45.10%\n",
      "Epoch: 0.16, Train Accuracy: 63.66%, Validation Accuracy: 63.68%\n",
      "Epoch: 0.32, Train Accuracy: 70.27%, Validation Accuracy: 68.84%\n",
      "Epoch: 0.48, Train Accuracy: 77.89%, Validation Accuracy: 75.84%\n",
      "Epoch: 0.64, Train Accuracy: 81.34%, Validation Accuracy: 79.36%\n",
      "Epoch: 0.80, Train Accuracy: 83.88%, Validation Accuracy: 81.20%\n",
      "Epoch: 0.96, Train Accuracy: 85.31%, Validation Accuracy: 82.08%\n",
      "Epoch: 1.00, Train Accuracy: 85.58%, Validation Accuracy: 82.18%\n",
      "Epoch: 1.16, Train Accuracy: 86.58%, Validation Accuracy: 83.06%\n",
      "Epoch: 1.32, Train Accuracy: 87.25%, Validation Accuracy: 83.18%\n",
      "Epoch: 1.48, Train Accuracy: 87.76%, Validation Accuracy: 84.36%\n",
      "Epoch: 1.64, Train Accuracy: 88.52%, Validation Accuracy: 84.38%\n",
      "Epoch: 1.80, Train Accuracy: 89.06%, Validation Accuracy: 84.80%\n",
      "Epoch: 1.96, Train Accuracy: 89.55%, Validation Accuracy: 85.16%\n",
      "Epoch: 2.00, Train Accuracy: 89.56%, Validation Accuracy: 85.08%\n",
      "Epoch: 2.16, Train Accuracy: 89.94%, Validation Accuracy: 85.56%\n",
      "Epoch: 2.32, Train Accuracy: 90.44%, Validation Accuracy: 85.28%\n",
      "Epoch: 2.48, Train Accuracy: 90.85%, Validation Accuracy: 85.34%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2.64, Train Accuracy: 91.13%, Validation Accuracy: 85.66%\n",
      "Epoch: 2.80, Train Accuracy: 91.48%, Validation Accuracy: 85.76%\n",
      "Epoch: 2.96, Train Accuracy: 91.83%, Validation Accuracy: 85.54%\n",
      "Epoch: 3.00, Train Accuracy: 91.88%, Validation Accuracy: 85.28%\n",
      "Epoch: 3.16, Train Accuracy: 92.08%, Validation Accuracy: 85.32%\n",
      "Epoch: 3.32, Train Accuracy: 92.12%, Validation Accuracy: 85.94%\n",
      "Epoch: 3.48, Train Accuracy: 92.58%, Validation Accuracy: 85.24%\n",
      "Epoch: 3.64, Train Accuracy: 92.72%, Validation Accuracy: 85.56%\n",
      "Epoch: 3.80, Train Accuracy: 93.06%, Validation Accuracy: 85.34%\n",
      "Epoch: 3.96, Train Accuracy: 93.05%, Validation Accuracy: 85.78%\n",
      "Epoch: 4.00, Train Accuracy: 93.31%, Validation Accuracy: 85.42%\n",
      "Epoch: 4.16, Train Accuracy: 93.47%, Validation Accuracy: 85.32%\n",
      "Epoch: 4.32, Train Accuracy: 93.61%, Validation Accuracy: 85.22%\n",
      "Epoch: 4.48, Train Accuracy: 93.72%, Validation Accuracy: 85.10%\n",
      "Epoch: 4.64, Train Accuracy: 94.03%, Validation Accuracy: 84.96%\n",
      "Epoch: 4.80, Train Accuracy: 94.23%, Validation Accuracy: 84.94%\n",
      "Epoch: 4.96, Train Accuracy: 94.17%, Validation Accuracy: 85.44%\n",
      "Epoch: 5.00, Train Accuracy: 94.47%, Validation Accuracy: 84.98%\n",
      "Epoch: 5.16, Train Accuracy: 94.53%, Validation Accuracy: 84.86%\n",
      "Epoch: 5.32, Train Accuracy: 94.72%, Validation Accuracy: 84.90%\n",
      "Epoch: 5.48, Train Accuracy: 94.76%, Validation Accuracy: 84.90%\n",
      "Epoch: 5.64, Train Accuracy: 94.96%, Validation Accuracy: 84.70%\n",
      "Epoch: 5.80, Train Accuracy: 95.22%, Validation Accuracy: 84.94%\n",
      "Epoch: 5.96, Train Accuracy: 95.33%, Validation Accuracy: 84.60%\n",
      "Epoch: 6.00, Train Accuracy: 95.34%, Validation Accuracy: 84.54%\n",
      "Epoch: 6.16, Train Accuracy: 95.42%, Validation Accuracy: 84.50%\n",
      "Epoch: 6.32, Train Accuracy: 95.49%, Validation Accuracy: 84.52%\n",
      "Epoch: 6.48, Train Accuracy: 95.56%, Validation Accuracy: 84.42%\n",
      "Epoch: 6.64, Train Accuracy: 95.78%, Validation Accuracy: 84.20%\n",
      "Epoch: 6.80, Train Accuracy: 95.97%, Validation Accuracy: 84.10%\n",
      "Epoch: 6.96, Train Accuracy: 96.11%, Validation Accuracy: 84.30%\n",
      "Epoch: 7.00, Train Accuracy: 96.08%, Validation Accuracy: 84.20%\n",
      "Epoch: 7.16, Train Accuracy: 96.19%, Validation Accuracy: 84.40%\n",
      "Epoch: 7.32, Train Accuracy: 96.17%, Validation Accuracy: 83.80%\n",
      "Epoch: 7.48, Train Accuracy: 96.40%, Validation Accuracy: 84.24%\n",
      "Epoch: 7.64, Train Accuracy: 96.49%, Validation Accuracy: 84.00%\n",
      "Epoch: 7.80, Train Accuracy: 96.61%, Validation Accuracy: 83.78%\n",
      "Epoch: 7.96, Train Accuracy: 96.70%, Validation Accuracy: 83.46%\n",
      "Epoch: 8.00, Train Accuracy: 96.71%, Validation Accuracy: 83.40%\n",
      "Epoch: 8.16, Train Accuracy: 96.81%, Validation Accuracy: 83.58%\n",
      "Epoch: 8.32, Train Accuracy: 96.88%, Validation Accuracy: 83.18%\n",
      "Epoch: 8.48, Train Accuracy: 96.92%, Validation Accuracy: 83.24%\n",
      "Epoch: 8.64, Train Accuracy: 97.00%, Validation Accuracy: 83.02%\n",
      "Epoch: 8.80, Train Accuracy: 97.18%, Validation Accuracy: 83.34%\n",
      "Epoch: 8.96, Train Accuracy: 97.36%, Validation Accuracy: 83.28%\n",
      "Epoch: 9.00, Train Accuracy: 97.36%, Validation Accuracy: 83.34%\n",
      "Epoch: 9.16, Train Accuracy: 97.33%, Validation Accuracy: 83.42%\n",
      "Epoch: 9.32, Train Accuracy: 97.25%, Validation Accuracy: 82.86%\n",
      "Epoch: 9.48, Train Accuracy: 97.43%, Validation Accuracy: 82.86%\n",
      "Epoch: 9.64, Train Accuracy: 97.58%, Validation Accuracy: 83.20%\n",
      "Epoch: 9.80, Train Accuracy: 97.67%, Validation Accuracy: 83.20%\n",
      "Epoch: 9.96, Train Accuracy: 97.72%, Validation Accuracy: 82.80%\n",
      "Epoch: 10.00, Train Accuracy: 97.80%, Validation Accuracy: 82.96%\n"
     ]
    }
   ],
   "source": [
    "# run tokenization experiments \n",
    "\n",
    "tokenization_experiments = {}\n",
    "\n",
    "for t_scheme in tokenization_schemes: \n",
    "    \n",
    "    print (\"Training with and {} tokenization scheme...\".format(t_scheme))\n",
    "    \n",
    "    # load tokens from disk \n",
    "    train_data_tokens, val_data_tokens, test_data_tokens = load_tokens_from_disk(folder_name=t_scheme)\n",
    "    \n",
    "    # train and evaluate     \n",
    "    results, _ = run_experiment(train_data_tokens, val_data_tokens, train_labels, val_labels,\n",
    "                                max_sentence_length=200, max_vocab_size=10000, emb_dim=100, \n",
    "                                optimizer=Adam, learning_rate=0.001, num_epochs=10, batch_size=32, print_results=True) \n",
    "    \n",
    "    # store to dict \n",
    "    tokenization_experiments[t_scheme] = results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'lowercase_no_punc_stopwords_nltk' tokenization scheme yields the best performance on our validation set. We shall proceed with this tokenization scheme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lowercase_no_punc_stopwords_nltk</td>\n",
       "      <td>86.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lowercase_no_punc_stopwords_spacy</td>\n",
       "      <td>86.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lowercase_no_punc_stopwords_nltk_lemmatize</td>\n",
       "      <td>85.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lowercase_no_punc_stopwords_spacy_lemmatize</td>\n",
       "      <td>85.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lowercase_no_punc</td>\n",
       "      <td>85.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lowercase</td>\n",
       "      <td>84.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         model  val_acc\n",
       "3             lowercase_no_punc_stopwords_nltk    86.68\n",
       "2            lowercase_no_punc_stopwords_spacy    86.48\n",
       "5   lowercase_no_punc_stopwords_nltk_lemmatize    85.94\n",
       "4  lowercase_no_punc_stopwords_spacy_lemmatize    85.22\n",
       "1                            lowercase_no_punc    85.14\n",
       "0                                    lowercase    84.70"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_val_accuracy(tokenization_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_learning_curve(tokenization_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we test various combinations of model hyperparameters, namely max_sentence_length, max_ngram, max_vocab_size, emb_dim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default to using the tokenization scheme that won previously \n",
    "SELECT_TOKENIZATION_SCHEME = 'lowercase_no_punc_stopwords_nltk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's get a sense of the distribution of word length amongst the reviews in train set \n",
    "word_lengths = pd.Series([len(x.split(' ')) for x in train_data])\n",
    "print(word_lengths.describe())\n",
    "word_lengths.hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Accuracy: 51.13%, Validation Accuracy: 52.16%\n",
      "Epoch: 0.16, Train Accuracy: 83.27%, Validation Accuracy: 81.82%\n",
      "Epoch: 0.32, Train Accuracy: 87.89%, Validation Accuracy: 85.52%\n",
      "Epoch: 0.48, Train Accuracy: 89.80%, Validation Accuracy: 85.14%\n",
      "Epoch: 0.64, Train Accuracy: 91.82%, Validation Accuracy: 86.52%\n",
      "Epoch: 0.80, Train Accuracy: 93.19%, Validation Accuracy: 87.02%\n",
      "Epoch: 0.96, Train Accuracy: 94.00%, Validation Accuracy: 86.98%\n",
      "Epoch: 1.00, Train Accuracy: 94.19%, Validation Accuracy: 87.02%\n",
      "Epoch: 1.16, Train Accuracy: 94.11%, Validation Accuracy: 85.90%\n",
      "Epoch: 1.32, Train Accuracy: 94.83%, Validation Accuracy: 87.42%\n",
      "Epoch: 1.48, Train Accuracy: 95.22%, Validation Accuracy: 86.94%\n",
      "Epoch: 1.64, Train Accuracy: 95.77%, Validation Accuracy: 86.02%\n",
      "Epoch: 1.80, Train Accuracy: 96.10%, Validation Accuracy: 85.74%\n",
      "Epoch: 1.96, Train Accuracy: 96.71%, Validation Accuracy: 86.64%\n",
      "Epoch: 2.00, Train Accuracy: 96.62%, Validation Accuracy: 86.20%\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment(train_data_tokens, val_data_tokens, train_labels, val_labels, optimizer=Adam, \n",
    "                         max_sentence_length=np.array([len(x) for x in train_data_indices]).max(), \n",
    "                         learning_rate=0.01, emb_dim=100, max_vocab_size=10000, \n",
    "                         num_epochs=2, batch_size=32, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramify_datum(datum_tokens, max_ngram): \n",
    "    \"\"\" Generates n-grams up to max_ngram for one given list of tokens representing a datum \"\"\"\n",
    "    result = [] \n",
    "    n = max_ngram \n",
    "    \n",
    "    # decrement n to append ..., 3-grams, 2-grams to result \n",
    "    while n >= 1: \n",
    "        n_grams = [\" \".join(item) for item in list(zip(*[datum_tokens[i:] for i in range(n)]))]\n",
    "        result = result + n_grams \n",
    "        n = n - 1 \n",
    "        \n",
    "    # when n=1 just append original tokens\n",
    "    result = result + datum_tokens  \n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_ngramify_dataset(tokenization_scheme, max_ngram): \n",
    "    \"\"\" Loads train/val/test 1-gram tokens from disk for a given tokenization scheme, \n",
    "        returns the ngrammified dataset \"\"\" \n",
    "    \n",
    "    # load unigram tokens from disk \n",
    "    train_data_tokens, val_data_tokens, test_data_tokens = load_tokens_from_disk(folder_name=tokenization_scheme)\n",
    "    \n",
    "    # ngrammify \n",
    "    train_ngrams = [ngramify(datum, max_ngram) for datum in train_data_tokens]  \n",
    "    val_ngrams = [ngramify(datum, max_ngram) for datum in val_data_tokens] \n",
    "    test_ngrams = [ngramify(datum, max_ngram) for datum in test_data_tokens]  \n",
    "    \n",
    "    return train_ngrams, val_ngrams, test_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model hyperparameters to try \n",
    "max_ngrams = [1, 2, 3, 4] \n",
    "\n",
    "# default hyperparameters to use \n",
    "EMB_DIM = 100\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 2 \n",
    "\n",
    "# max_vocab_sizes = [10000, 50000, 100000, 250000, 500000] \n",
    "# emb_dims = [50, 100, 250, 500, 1000]\n",
    "\n",
    "# max_ngrams = [1, 2] \n",
    "# max_vocab_sizes = [250000, 500000] \n",
    "# emb_dims = [500, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_experiments = {}\n",
    "# num_experiments = len(max_ngrams) * len(max_vocab_sizes) * len(emb_dims)\n",
    "\n",
    "# for max_ngram in max_ngrams:\n",
    "#     # load and ngramify tokens from selected tokenization scheme \n",
    "#     train_ngrams, val_ngrams, _ = load_and_ngramify_dataset(SELECT_TOKENIZATION_SCHEME, max_ngram)\n",
    "\n",
    "#     for max_vocab_size in max_vocab_sizes: \n",
    "#         for emb_dim in emb_dims: \n",
    "                  \n",
    "#             # train and evaluate \n",
    "#             results = run_experiment(train_ngrams, val_ngrams, train_labels, val_labels, optimizer=Adam, \n",
    "#                                      learning_rate=0.01, emb_dim=emb_dim, max_vocab_size=max_vocab_size, \n",
    "#                                      max_sentence_length=200, num_epochs=5, batch_size=32, print_results=False)\n",
    "            \n",
    "#             # store to dict and save to pickle \n",
    "#             hyparams = str({'max_ngram': max_ngram, 'max_vocab_size': max_vocab_size, 'emb_dim': emb_dim})\n",
    "#             model_hyparams_experiments[hyparams] = results \n",
    "\n",
    "#             # print and report results of each experiment \n",
    "#             print(\"Experiment [{}/{}]: {} completed with {:.1f}% accuracy\".format(\n",
    "#                 len(model_hyparams_experiments), num_experiments, hyparams, \n",
    "#                 pd.DataFrame.from_dict(results)['val_acc'].max()))\n",
    "\n",
    "# # save results to pickle \n",
    "# current_dt = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "# results_filename = 'experiment_results/model_hyperparameter_experiments_{}.csv'.format(current_dt)\n",
    "# save_to_pickle(model_hyparams_experiments, results_filename) \n",
    "# print(\"Finished: Results saved to {}\".format(results_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment [1/100]: {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 50} completed with 84.1% accuracy\n",
      "Experiment [2/100]: {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 100} completed with 84.2% accuracy\n",
      "Experiment [3/100]: {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 250} completed with 83.8% accuracy\n",
      "Experiment [4/100]: {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 500} completed with 83.4% accuracy\n",
      "Experiment [5/100]: {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 1000} completed with 82.8% accuracy\n",
      "Experiment [6/100]: {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 50} completed with 83.5% accuracy\n",
      "Experiment [7/100]: {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 100} completed with 83.5% accuracy\n",
      "Experiment [8/100]: {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 250} completed with 83.2% accuracy\n",
      "Experiment [9/100]: {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 500} completed with 82.8% accuracy\n",
      "Experiment [10/100]: {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 1000} completed with 82.5% accuracy\n",
      "Experiment [11/100]: {'max_ngram': 1, 'max_vocab_size': 100000, 'emb_dim': 50} completed with 84.5% accuracy\n",
      "Experiment [12/100]: {'max_ngram': 1, 'max_vocab_size': 100000, 'emb_dim': 100} completed with 83.9% accuracy\n",
      "Experiment [13/100]: {'max_ngram': 1, 'max_vocab_size': 100000, 'emb_dim': 250} completed with 83.5% accuracy\n",
      "Experiment [14/100]: {'max_ngram': 1, 'max_vocab_size': 100000, 'emb_dim': 500} completed with 82.9% accuracy\n",
      "Experiment [15/100]: {'max_ngram': 1, 'max_vocab_size': 100000, 'emb_dim': 1000} completed with 82.1% accuracy\n",
      "Experiment [16/100]: {'max_ngram': 1, 'max_vocab_size': 250000, 'emb_dim': 50} completed with 84.4% accuracy\n",
      "Experiment [17/100]: {'max_ngram': 1, 'max_vocab_size': 250000, 'emb_dim': 100} completed with 83.9% accuracy\n",
      "Experiment [18/100]: {'max_ngram': 1, 'max_vocab_size': 250000, 'emb_dim': 250} completed with 83.8% accuracy\n",
      "Experiment [19/100]: {'max_ngram': 1, 'max_vocab_size': 250000, 'emb_dim': 500} completed with 83.1% accuracy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-094ca4353ac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             results = run_experiment(train_ngrams, val_ngrams, train_labels, val_labels, optimizer=Adam, \n\u001b[1;32m     15\u001b[0m                                      \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                      max_sentence_length=200, num_epochs=5, batch_size=32, print_results=False)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# store to dict and save to pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-187-672002b5ec42>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(train_tokens, val_tokens, train_labels, val_labels, optimizer, learning_rate, emb_dim, max_vocab_size, max_sentence_length, num_epochs, batch_size, print_results)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-cb9de2c8e8cd>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(model, optimizer, train_loader, val_loader, num_epochs, print_results)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-679cdda7f5ef>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(loader, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/nlpclass/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-6f71ba51ff07>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, length)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \"\"\"\n\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run grid search on model hyperparameters \n",
    "\n",
    "# model_hyparams_experiments = {}\n",
    "# num_experiments = len(max_ngrams) * len(max_vocab_sizes) * len(emb_dims)\n",
    "\n",
    "# for max_ngram in max_ngrams:\n",
    "#     # load and ngramify tokens from selected tokenization scheme \n",
    "#     train_ngrams, val_ngrams, _ = load_and_ngramify_dataset(SELECT_TOKENIZATION_SCHEME, max_ngram)\n",
    "\n",
    "#     for max_vocab_size in max_vocab_sizes: \n",
    "#         for emb_dim in emb_dims: \n",
    "                  \n",
    "#             # train and evaluate \n",
    "#             results = run_experiment(train_ngrams, val_ngrams, train_labels, val_labels, optimizer=Adam, \n",
    "#                                      learning_rate=0.01, emb_dim=emb_dim, max_vocab_size=max_vocab_size, \n",
    "#                                      max_sentence_length=200, num_epochs=5, batch_size=32, print_results=False)\n",
    "            \n",
    "#             # store to dict and save to pickle \n",
    "#             hyparams = str({'max_ngram': max_ngram, 'max_vocab_size': max_vocab_size, 'emb_dim': emb_dim})\n",
    "#             model_hyparams_experiments[hyparams] = results \n",
    "\n",
    "#             # print and report results of each experiment \n",
    "#             print(\"Experiment [{}/{}]: {} completed with {:.1f}% accuracy\".format(\n",
    "#                 len(model_hyparams_experiments), num_experiments, hyparams, \n",
    "#                 pd.DataFrame.from_dict(results)['val_acc'].max()))\n",
    "\n",
    "# # save results to pickle \n",
    "# current_dt = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "# results_filename = 'experiment_results/model_hyperparameter_experiments_{}.csv'.format(current_dt)\n",
    "# save_to_pickle(model_hyparams_experiments, results_filename) \n",
    "# print(\"Finished: Results saved to {}\".format(results_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>max_ngram</th>\n",
       "      <th>max_vocab_size</th>\n",
       "      <th>emb_dim</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>84.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 50}</td>\n",
       "      <td>1</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>84.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'max_ngram': 2, 'max_vocab_size': 10000, 'emb_dim': 50}</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>50</td>\n",
       "      <td>84.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'max_ngram': 2, 'max_vocab_size': 10000, 'emb_dim': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>84.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'max_ngram': 2, 'max_vocab_size': 50000, 'emb_dim': 50}</td>\n",
       "      <td>2</td>\n",
       "      <td>50000</td>\n",
       "      <td>50</td>\n",
       "      <td>83.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 100}</td>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "      <td>100</td>\n",
       "      <td>83.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 50}</td>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "      <td>50</td>\n",
       "      <td>83.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'max_ngram': 2, 'max_vocab_size': 50000, 'emb_dim': 100}</td>\n",
       "      <td>2</td>\n",
       "      <td>50000</td>\n",
       "      <td>100</td>\n",
       "      <td>83.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       model  max_ngram  \\\n",
       "1  {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 100}          1   \n",
       "0   {'max_ngram': 1, 'max_vocab_size': 10000, 'emb_dim': 50}          1   \n",
       "4   {'max_ngram': 2, 'max_vocab_size': 10000, 'emb_dim': 50}          2   \n",
       "5  {'max_ngram': 2, 'max_vocab_size': 10000, 'emb_dim': 100}          2   \n",
       "6   {'max_ngram': 2, 'max_vocab_size': 50000, 'emb_dim': 50}          2   \n",
       "3  {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 100}          1   \n",
       "2   {'max_ngram': 1, 'max_vocab_size': 50000, 'emb_dim': 50}          1   \n",
       "7  {'max_ngram': 2, 'max_vocab_size': 50000, 'emb_dim': 100}          2   \n",
       "\n",
       "   max_vocab_size  emb_dim  val_acc  \n",
       "1           10000      100    84.28  \n",
       "0           10000       50    84.22  \n",
       "4           10000       50    84.10  \n",
       "5           10000      100    84.08  \n",
       "6           50000       50    83.98  \n",
       "3           50000      100    83.92  \n",
       "2           50000       50    83.86  \n",
       "7           50000      100    83.38  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = 'experiment_results/model_hyperparameter_experiments_2018-10-04_22_42_17.csv'\n",
    "model_hyparams_experiments = pkl.load(open(results_filename, \"rb\"))\n",
    "model_hyparams_val_acc = print_val_accuracy(model_hyparams_experiments, split_name=True)  \n",
    "model_hyparams_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_algos = {'Adam': Adam, 'SGD': SGD, 'RMSprop': RMSprop}\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "\n",
    "EMB_DIM = 100 \n",
    "MAX_VOCAB_SIZE = 10000 \n",
    "MAX_NGRAM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngrams, val_ngrams, _ = load_and_ngramify_dataset(SELECT_TOKENIZATION_SCHEME, max_ngram=MAX_NGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment [1/9]: {'optim_algo': 'Adam', 'learning_rate': 0.1} completed with 82.1% accuracy\n",
      "Experiment [2/9]: {'optim_algo': 'Adam', 'learning_rate': 0.01} completed with 84.3% accuracy\n",
      "Experiment [3/9]: {'optim_algo': 'Adam', 'learning_rate': 0.001} completed with 77.3% accuracy\n",
      "Experiment [4/9]: {'optim_algo': 'SGD', 'learning_rate': 0.1} completed with 64.2% accuracy\n",
      "Experiment [5/9]: {'optim_algo': 'SGD', 'learning_rate': 0.01} completed with 58.1% accuracy\n",
      "Experiment [6/9]: {'optim_algo': 'SGD', 'learning_rate': 0.001} completed with 49.7% accuracy\n",
      "Experiment [7/9]: {'optim_algo': 'RMSprop', 'learning_rate': 0.1} completed with 83.9% accuracy\n",
      "Experiment [8/9]: {'optim_algo': 'RMSprop', 'learning_rate': 0.01} completed with 84.0% accuracy\n",
      "Experiment [9/9]: {'optim_algo': 'RMSprop', 'learning_rate': 0.001} completed with 78.4% accuracy\n",
      "Finished: Results saved to experiment_results/optim_hyperparameter_experiments_2018-10-04_23_44_23.csv\n"
     ]
    }
   ],
   "source": [
    "# run grid search on model hyperparameters \n",
    "\n",
    "optim_hyparams_experiments = {}\n",
    "num_experiments = len(optim_algos) * len(learning_rates)\n",
    "\n",
    "for algo in optim_algos: \n",
    "    for lr in learning_rates: \n",
    "\n",
    "        # train and evaluate \n",
    "        results = run_experiment(train_ngrams, val_ngrams, train_labels, val_labels, optimizer=optim_algos[algo], \n",
    "                                 learning_rate=lr, emb_dim=EMB_DIM, max_vocab_size=MAX_VOCAB_SIZE, \n",
    "                                 max_sentence_length=200, num_epochs=1, batch_size=32, print_results=False)\n",
    "\n",
    "        # store to dict and save to pickle \n",
    "        hyparams = str({'optim_algo': algo, 'learning_rate': lr})\n",
    "        optim_hyparams_experiments[hyparams] = results \n",
    "\n",
    "        # print and report results of each experiment \n",
    "        print(\"Experiment [{}/{}]: {} completed with {:.1f}% accuracy\".format(\n",
    "            len(optim_hyparams_experiments), num_experiments, hyparams, \n",
    "            pd.DataFrame.from_dict(results)['val_acc'].max()))\n",
    "\n",
    "# save results to pickle \n",
    "current_dt = datetime.now().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "results_filename = 'experiment_results/optim_hyperparameter_experiments_{}.csv'.format(current_dt)\n",
    "save_to_pickle(optim_hyparams_experiments, results_filename) \n",
    "print(\"Finished: Results saved to {}\".format(results_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
