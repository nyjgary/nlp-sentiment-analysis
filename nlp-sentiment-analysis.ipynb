{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time \n",
    "import os\n",
    "from os import listdir \n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "from functools import partial \n",
    "import nltk\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to load reviews from directories \n",
    "\n",
    "def load_single_review(fdir, fname): \n",
    "    \"\"\" Takes as input file directory and file name of a single review, returns review as string \"\"\"\n",
    "    fpath = fdir + '/' + fname \n",
    "    with open(fpath, 'r') as f: \n",
    "        review = f.read()\n",
    "        return review \n",
    "    \n",
    "def load_dir_reviews(fdir): \n",
    "    \"\"\" Takes as input file directory where reviews are stored, returns them as a list of review strings \"\"\"\n",
    "    fnames = [f for f in listdir(fdir)]\n",
    "    reviews = [load_single_review(fdir, fname) for fname in fnames]\n",
    "    return reviews\n",
    "\n",
    "def combine_data(neg_reviews, pos_reviews): \n",
    "    \"\"\" Combines lists of negative and positive reviews, returns a combined dataset comprising reviews and labels \"\"\"\n",
    "    neg_with_labels = [(review, 0) for review in neg_reviews] \n",
    "    pos_with_labels = [(review, 1) for review in pos_reviews]\n",
    "    combined = neg_with_labels + pos_with_labels\n",
    "    combined = random.sample(combined, len(combined))\n",
    "    reviews = [comb[0] for comb in combined]\n",
    "    labels = [comb[1] for comb in combined]\n",
    "    return reviews, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews into lists \n",
    "train_val_neg = load_dir_reviews('aclImdb/train/neg')\n",
    "train_val_pos = load_dir_reviews('aclImdb/train/pos')\n",
    "test_neg = load_dir_reviews('aclImdb/test/neg')\n",
    "test_pos = load_dir_reviews('aclImdb/test/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split train into train vs. validation sets \n",
    "train_split = int(20000 / 2) \n",
    "train_neg = train_val_neg[:train_split]\n",
    "train_pos = train_val_pos[:train_split]\n",
    "val_neg = train_val_neg[train_split:]\n",
    "val_pos = train_val_pos[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# combine pos and neg reviews to get unified datasets \n",
    "train_data, train_labels = combine_data(train_neg, train_pos)\n",
    "val_data, val_labels = combine_data(val_neg, val_pos)\n",
    "test_data, test_labels = combine_data(test_neg, test_pos)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews \n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation \n",
    "spacy_stop_words = tokenizer.Defaults.stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def lower_case(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed]\n",
    "\n",
    "def tokenize_dataset(dataset, processing_func):\n",
    "    \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "        a list comprising all the tokens from the dataset \"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "                                               disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = processing_func(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset\n",
    "\n",
    "def tokenize_dataset_to_disk(dataset, processing_func, filename): \n",
    "    \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "    start_time = time.time() \n",
    "    token_dataset = tokenize_dataset(dataset, processing_func)\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    with open(filename, \"wb\") as f: \n",
    "        pkl.dump(token_dataset, f)\n",
    "    time_elapsed = (time.time() - start_time) / 60.0 \n",
    "    print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(filename, time_elapsed))\n",
    "    return token_dataset\n",
    "    \n",
    "def tokenize_datasets_to_disk(train_data, val_data, test_data, processing_func, folder_name): \n",
    "    \"\"\" Tokenizes train, val, test datasets and save as pickle to data subfolder \n",
    "        Also returns tokenized datasets\n",
    "    \"\"\"\n",
    "    train_data_tokens = tokenize_dataset_to_disk(train_data, processing_func, \n",
    "                                                 \"data/{}/train_data_tokens.p\".format(folder_name))\n",
    "    val_data_tokens = tokenize_dataset_to_disk(val_data, processing_func, \n",
    "                                               \"data/{}/val_data_tokens.p\".format(folder_name))    \n",
    "    test_data_tokens = tokenize_dataset_to_disk(test_data, processing_func, \n",
    "                                                \"data/{}/test_data_tokens.p\".format(folder_name))\n",
    "    return train_data_tokens, val_data_tokens, test_data_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3fd7ba874946f086db20206f73fcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/standard/train_data_tokens.p in 0.6 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed89983322c744b78a8941158e12c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/standard/val_data_tokens.p in 0.1 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb5328686414704a5be7fc9fa455891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/standard/test_data_tokens.p in 0.8 minutes\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens, val_data_tokens, test_data_tokens = tokenize_datasets_to_disk(\n",
    "    train_data, val_data, test_data, processing_func=lower_case, folder_name='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# load saved tokens \n",
    "train_data_tokens = pkl.load(open(\"data/standard/train_data_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"data/standard/val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"data/standard/test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double check \n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from 10000 most common tokens in the training set \n",
    "\n",
    "PAD_IDX=0\n",
    "UNK_IDX=1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size=10000, pad_idx=PAD_IDX, unk_idx=UNK_IDX): \n",
    "    \"\"\" Takes list of all tokens and returns:\n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    token2id['<pad>'] = pad_idx\n",
    "    token2id['<unk>'] = unk_idx\n",
    "    return token2id, id2token \n",
    "    \n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 9982 ; token gillian\n",
      "Token gillian; token id 9982\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset \n",
    "\n",
    "def token2index_dataset(tokens_data, token2id, unk_idx=UNK_IDX): \n",
    "    \"\"\" Converts data from word tokens to token indices \"\"\"\n",
    "    indices_data = []\n",
    "    for datum in tokens_data: \n",
    "        index_list = [token2id[token] if token in token2id else unk_idx for token in datum]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data \n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "test_data_indices = token2index_dataset(test_data_tokens, token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# check size of data \n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'disliked', 'this', 'film', 'intensely', 'and', 'left', 'during', 'the', 'scene', 'where', 'the', 'loyalist', 'gang', 'are', 'shot', 'up', 'by', 'the', 'british', '.', 'the', 'film', 'effectively', 'blames', 'the', 'people', 'of', 'ni', 'as', 'being', 'the', 'cause', 'of', 'their', 'own', 'troubles', '.', 'it', 'suggests', 'that', 'the', '25', 'year', 'war', 'was', 'a', 'question', 'of', 'intransigence', 'and', 'nothing', 'to', 'do', 'with', 'britain', \"'s\", 'partition', 'of', 'ireland', 'and', 'domination', 'of', 'its', 'history', 'i.e.', 'ni', 'was', 'created', 'by', 'britain', 'in', '1921', 'irrespective', 'of', 'the', 'wishes', 'of', 'the', 'rest', 'of', 'ireland.<br', '/><br', '/>the', 'characters', 'are', 'portrayed', 'as', 'hapless', 'fools', ',', 'even', 'though', 'i', 'despise', 'loyalist', 'paramilitaries', 'they', 'were', 'fighting', 'for', 'a', 'cause', '-', 'maintaining', 'their', 'artificial', 'privileges', 'over', 'the', 'catholic', 'community', '.', 'it', 'is', 'a', 'known', 'fact', 'that', 'british', 'intelligence', 'collaborated', 'with', 'loyalists', 'during', 'the', 'war', ',', 'no', 'doubt', 'to', 'keep', 'the', 'catholics', 'at', 'bay', 'and', 'demoralise', 'republicanism.<br', '/><br', '/>nineties', \"'\", 'values', 'about', \"'\", 'machismo', \"'\", ',', 'masculinity', 'etc', 'are', 'transposed', 'on', 'to', '1970s', 'belfast', 'and', 'are', 'portrayed', 'as', 'part', 'of', 'the', 'supposedly', 'unique', 'irish', \"'\", 'psyche', \"'\", 'which', 'leads', 'to', 'violence', '.', 'the', 'stupid', 'song', 'from', 'the', 'woman', 'in', 'the', 'club', '-', 'old', 'ireland', 'of', 'green', 'fields', '..', 'blah', '..', 'blah', '..', '-', 'is', 'given', 'a', 'symbolic', 'stature', ',', 'i.e.', 'poor', 'young', 'fools', 'fighting', 'for', 'an', 'impossible', 'cause', '.', 'tedious', ',', 'ahistorical', ',', 'cheap', 'and', 'nasty', 'trash', '.', \"o'sullivan\", 'has', 'made', 'a', 'personal', 'statement', 'on', 'a', 'conflict', 'which', 'requires', 'serious', 'political', 'analysis', '.']\n",
      "[12, 5441, 13, 25, 8019, 5, 322, 336, 2, 151, 136, 2, 1, 1349, 32, 345, 67, 41, 2, 654, 4, 2, 25, 2700, 7859, 2, 94, 7, 1, 20, 124, 2, 1162, 7, 80, 220, 5004, 4, 10, 3330, 14, 2, 2939, 328, 334, 19, 6, 896, 7, 1, 5, 178, 8, 55, 22, 3266, 16, 8875, 7, 4803, 5, 1, 7, 106, 466, 2405, 1, 19, 1083, 41, 3266, 11, 1, 1, 7, 2, 3039, 7, 2, 364, 7, 1, 18, 117, 119, 32, 995, 20, 5796, 7186, 3, 70, 170, 12, 7722, 1, 1, 40, 79, 983, 21, 6, 1162, 17, 9287, 80, 4599, 1, 137, 2, 3008, 1873, 4, 10, 9, 6, 562, 203, 14, 654, 1621, 1, 22, 1, 336, 2, 334, 3, 68, 806, 8, 406, 2, 1, 39, 5129, 5, 1, 1, 18, 1, 54, 1258, 52, 54, 1, 54, 3, 1, 566, 32, 1, 26, 8, 3560, 1, 5, 32, 995, 20, 189, 7, 2, 1584, 963, 2830, 54, 6952, 54, 73, 850, 8, 595, 4, 2, 403, 610, 45, 2, 253, 11, 2, 1325, 17, 164, 4803, 7, 1343, 4748, 413, 2641, 413, 2641, 413, 17, 9, 360, 6, 5247, 9748, 3, 2405, 358, 198, 7186, 983, 21, 43, 1193, 1162, 4, 2351, 3, 1, 3, 707, 5, 1678, 1179, 4, 9076, 53, 105, 6, 915, 2690, 26, 6, 1987, 73, 3380, 641, 986, 5005, 4]\n"
     ]
    }
   ],
   "source": [
    "# check tokenization of dataset \n",
    "print(train_data_tokens[0])\n",
    "print(train_data_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture and Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class and collate function for data loader \n",
    "\n",
    "class MovieReviewsDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list, max_sentence_length): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of movie review tokens and a list of labels \n",
    "        \"\"\"\n",
    "        self.data_list = data_list \n",
    "        self.label_list = label_list \n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        assert (len(self.data_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, length of list, and label of the data point\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:self.max_sentence_length]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "def collate_func(max_sentence_length, batch): \n",
    "    \"\"\" \n",
    "    Customized function for DataLoader that dynamically pads the batch so that the data have the same length\n",
    "    Note that this takes max_sentence_length as a first argument to be prefilled with a partial function later \n",
    "        to facilitate hyperparameter tuning \n",
    "    \"\"\"\n",
    "    data_list = [] \n",
    "    label_list = [] \n",
    "    length_list = [] \n",
    "    \n",
    "    for datum in batch:         \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        # pad data before appending \n",
    "        padded_vec = np.pad(array = np.array(datum[0]), \n",
    "                            pad_width = ((0, max_sentence_length - datum[1])), \n",
    "                            mode = 'constant', constant_values = 0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model architecture and helper methods \n",
    "\n",
    "class BagOfWords(nn.Module): \n",
    "    \"\"\" \n",
    "    BagOfWords classification model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim): \n",
    "        \"\"\" \n",
    "        @param vocab_size: size of the vocabulary \n",
    "        @param emd_dim: size of the word embedding \n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "        \n",
    "    def forward(self, data, length): \n",
    "        \"\"\" \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a review\n",
    "            that is represented using n-gram index. Note that they are padded to have the same length. \n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (i.e. non-padded)\n",
    "            length of each sentence in the data \n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "        \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to train and test model \n",
    "\n",
    "def test_model(loader, model): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    model.eval() \n",
    "    \n",
    "    for data_batch, length_batch, label_batch in loader: \n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predictions = outputs.max(1, keepdim=True)[1]    \n",
    "        total += label_batch.size(0)\n",
    "        correct += predictions.eq(label_batch.view_as(predictions)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, optimizer, train_loader, val_loader, num_epochs, print_results=True):  \n",
    "    \"\"\"\n",
    "    Trains model on data from train_loader and evaluates on data from val_loader for num_epochs \n",
    "    Returns results as a dictionary comprising epoch, train accuracy, and validation accuracy \n",
    "    \"\"\"\n",
    "    # train and validate \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    results = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i % 100 == 0 or ((epoch==num_epochs-1) & (i==len(train_loader)-1)):\n",
    "                result = {} \n",
    "                result['epoch'] = epoch + i / len(train_loader)\n",
    "                result['train_acc'] = test_model(train_loader, model)\n",
    "                result['val_acc'] = test_model(val_loader, model)       \n",
    "                results.append(result)\n",
    "\n",
    "                if print_results: \n",
    "                    print('Epoch: {:.2f}, Train Accuracy: {:.2f}%, Validation Accuracy: {:.2f}%'.format(\n",
    "                        result['epoch'], result['train_acc'], result['val_acc']))\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_tokens, val_tokens, train_labels, val_labels, \n",
    "                   optimizer, learning_rate, emb_dim, max_vocab_size, max_sentence_length,\n",
    "                   num_epochs, batch_size=32, print_results=True): \n",
    "    \n",
    "    \"\"\" Wraps all processing, training and evaluation steps in a function to facilitate hyperparam tuning. \n",
    "        Note that the function takes as input tokenized data rather than raw data since there's significant \n",
    "        lag time in generating tokens.  \n",
    "    \"\"\"\n",
    "    \n",
    "    # build vocab based on max_vocab_size specified \n",
    "    all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "    token2id, id2token = build_vocab(all_train_tokens, max_vocab_size)\n",
    "    \n",
    "    # convert tokens to token indices \n",
    "    train_data_indices = token2index_dataset(train_data_tokens, token2id)\n",
    "    val_data_indices = token2index_dataset(val_data_tokens, token2id)\n",
    "    \n",
    "    # instantiate PyTorch Dataset object \n",
    "    train_dataset = MovieReviewsDataset(train_data_indices, train_labels, max_sentence_length)\n",
    "    val_dataset = MovieReviewsDataset(val_data_indices, val_labels, max_sentence_length)\n",
    "    \n",
    "    # create PyTorch DataLoader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                               collate_fn=partial(collate_func, max_sentence_length))\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                             collate_fn=partial(collate_func, max_sentence_length))\n",
    "    \n",
    "    # instantiate model and optimizer \n",
    "    model = BagOfWords(len(token2id), emb_dim)\n",
    "    optim = optimizer(params=model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # train and evaluate \n",
    "    results = train_and_eval(model, optim, train_loader, val_loader, num_epochs, print_results=True)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's train and evaluate a basic model with a set of arbitrary hyperparameters to make sure the code runs correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.00, Train Accuracy: 49.95%, Validation Accuracy: 49.96%\n",
      "Epoch: 0.16, Train Accuracy: 64.78%, Validation Accuracy: 63.18%\n",
      "Epoch: 0.32, Train Accuracy: 69.47%, Validation Accuracy: 68.48%\n",
      "Epoch: 0.48, Train Accuracy: 72.06%, Validation Accuracy: 71.18%\n",
      "Epoch: 0.64, Train Accuracy: 76.30%, Validation Accuracy: 73.98%\n",
      "Epoch: 0.80, Train Accuracy: 78.39%, Validation Accuracy: 75.24%\n",
      "Epoch: 0.96, Train Accuracy: 80.28%, Validation Accuracy: 77.32%\n",
      "Epoch: 1.00, Train Accuracy: 80.59%, Validation Accuracy: 77.54%\n",
      "Epoch: 1.16, Train Accuracy: 81.89%, Validation Accuracy: 78.66%\n",
      "Epoch: 1.32, Train Accuracy: 83.03%, Validation Accuracy: 79.80%\n",
      "Epoch: 1.48, Train Accuracy: 84.06%, Validation Accuracy: 80.44%\n",
      "Epoch: 1.64, Train Accuracy: 84.64%, Validation Accuracy: 81.02%\n",
      "Epoch: 1.80, Train Accuracy: 85.61%, Validation Accuracy: 81.66%\n",
      "Epoch: 1.96, Train Accuracy: 86.23%, Validation Accuracy: 82.00%\n",
      "Epoch: 2.00, Train Accuracy: 86.22%, Validation Accuracy: 81.50%\n",
      "Epoch: 2.16, Train Accuracy: 86.71%, Validation Accuracy: 81.82%\n",
      "Epoch: 2.32, Train Accuracy: 87.16%, Validation Accuracy: 82.80%\n",
      "Epoch: 2.48, Train Accuracy: 87.75%, Validation Accuracy: 82.88%\n",
      "Epoch: 2.64, Train Accuracy: 88.11%, Validation Accuracy: 83.16%\n",
      "Epoch: 2.80, Train Accuracy: 88.28%, Validation Accuracy: 83.58%\n",
      "Epoch: 2.96, Train Accuracy: 88.88%, Validation Accuracy: 83.50%\n",
      "Epoch: 3.00, Train Accuracy: 88.57%, Validation Accuracy: 83.38%\n",
      "Epoch: 3.16, Train Accuracy: 89.12%, Validation Accuracy: 83.94%\n",
      "Epoch: 3.32, Train Accuracy: 89.47%, Validation Accuracy: 84.02%\n",
      "Epoch: 3.48, Train Accuracy: 89.72%, Validation Accuracy: 83.82%\n",
      "Epoch: 3.64, Train Accuracy: 90.12%, Validation Accuracy: 83.84%\n",
      "Epoch: 3.80, Train Accuracy: 90.20%, Validation Accuracy: 84.04%\n",
      "Epoch: 3.96, Train Accuracy: 90.61%, Validation Accuracy: 84.16%\n",
      "Epoch: 4.00, Train Accuracy: 90.72%, Validation Accuracy: 84.10%\n",
      "Epoch: 4.16, Train Accuracy: 90.68%, Validation Accuracy: 84.28%\n",
      "Epoch: 4.32, Train Accuracy: 90.95%, Validation Accuracy: 84.24%\n",
      "Epoch: 4.48, Train Accuracy: 91.18%, Validation Accuracy: 83.90%\n",
      "Epoch: 4.64, Train Accuracy: 91.41%, Validation Accuracy: 84.40%\n",
      "Epoch: 4.80, Train Accuracy: 91.58%, Validation Accuracy: 83.84%\n",
      "Epoch: 4.96, Train Accuracy: 91.87%, Validation Accuracy: 84.18%\n",
      "Epoch: 5.00, Train Accuracy: 92.03%, Validation Accuracy: 84.58%\n"
     ]
    }
   ],
   "source": [
    "results = run_experiment(train_data_tokens, val_data_tokens, train_labels, val_labels, optimizer=Adam, \n",
    "                         learning_rate=0.001, emb_dim=100, max_vocab_size=10000, max_sentence_length=200,\n",
    "                         num_epochs=5, batch_size=32, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we were able to obtain pretty good results already: ~84% accuracy on unseen validation dataset. We will proceed to try different proprocessing and hyperparameters next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4b0ee6d8>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curve \n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df = results_df.set_index('epoch')\n",
    "results_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap tokenization process as a function \n",
    "\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation \n",
    "# spacy_stop_words = tokenizer.Defaults.stop_words\n",
    "# nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case_remove_punc_nltk_stopwords_lemmatize(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.lemma_.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.lemma_.lower() not in nltk_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc_nltk_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in nltk_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc_spacy_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in spacy_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramify(datum_tokens, max_n): \n",
    "    \"\"\" Generates n-grams up to max_n for one given list of tokens representing a datum \"\"\"\n",
    "    result = [] \n",
    "    n = max_n \n",
    "    \n",
    "    # decrement n to append ..., 3-grams, 2-grams to result \n",
    "    while n >= 1: \n",
    "        n_grams = [\" \".join(item) for item in list(zip(*[datum_tokens[i:] for i in range(n)]))]\n",
    "        result = result + n_grams \n",
    "        n = n - 1 \n",
    "        \n",
    "    # when n=1 just append original tokens\n",
    "    result = result + datum_tokens  \n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_dataset(train_tokens, val_tokens, test_tokens, max_n, max_vocab_size):\n",
    "    \"\"\" Takes as input: orignal 1-gram train/val/test tokenized datasets, max_n (for n-gram), max_vocab_size, \n",
    "        and returns: \n",
    "        - token2id, id2token \n",
    "        - train_data_indices, val_data_indices, test_data_indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate ngram tokens \n",
    "    train_ngram_tokens = [ngramify(datum, 3) for datum in train_tokens]  \n",
    "    val_ngram_tokens = [ngramify(datum, 3) for datum in val_tokens]  \n",
    "    test_ngram_tokens = [ngramify(datum, 3) for datum in test_tokens]  \n",
    "    all_train_ngram_tokens = [item for sublist in train_ngram_tokens for item in sublist] \n",
    "    \n",
    "    # build vocab \n",
    "    token2id, id2token = build_vocab(all_train_ngram_tokens, max_vocab_size)\n",
    "    \n",
    "    # convert tokens to indices \n",
    "    train_data_indices = token2index_dataset(train_ngram_tokens)\n",
    "    val_data_indices = token2index_dataset(val_ngram_tokens)\n",
    "    test_data_indices = token2index_dataset(test_ngram_tokens)\n",
    "    \n",
    "    return token2id, id2token, train_data_indices, val_data_indices, test_data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token, train_data_indices, val_data_indices, test_data_indices = generate_ngram_dataset(\n",
    "    train_data_tokens, val_data_tokens, test_data_tokens, max_n=2, max_vocab_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 2\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 0 \n",
    "results = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and (i+1) % 100 == 0:\n",
    "            total_steps = epoch * 625 + (i+1) \n",
    "            train_acc = test_model(train_loader, model) # report train accuracy \n",
    "            val_acc = test_model(val_loader, model) # report validation accuracy \n",
    "            result = {} \n",
    "            result['step'] = total_steps \n",
    "            result['train_acc'] = train_acc\n",
    "            result['val_acc'] = val_acc        \n",
    "            results.append(result)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Total Steps: {}, Validation Acc: {}, Train Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), total_steps, val_acc, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
