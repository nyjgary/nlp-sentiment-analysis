{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time \n",
    "from os import listdir \n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to load reviews from directories \n",
    "\n",
    "def load_single_review(fdir, fname): \n",
    "    \"\"\" Takes as input file directory and file name of a single review, returns review as string \"\"\"\n",
    "    fpath = fdir + '/' + fname \n",
    "    with open(fpath, 'r') as f: \n",
    "        review = f.read()\n",
    "        return review \n",
    "    \n",
    "def load_dir_reviews(fdir): \n",
    "    \"\"\" Takes as input file directory where reviews are stored, returns them as a list of review strings \"\"\"\n",
    "    fnames = [f for f in listdir(fdir)]\n",
    "    reviews = [load_single_review(fdir, fname) for fname in fnames]\n",
    "    return reviews\n",
    "\n",
    "def combine_data(neg_reviews, pos_reviews): \n",
    "    \"\"\" Combines lists of negative and positive reviews, returns a combined dataset comprising reviews and labels \"\"\"\n",
    "    neg_with_labels = [(review, 0) for review in neg_reviews] \n",
    "    pos_with_labels = [(review, 1) for review in pos_reviews]\n",
    "    combined = neg_with_labels + pos_with_labels\n",
    "    combined = random.sample(combined, len(combined))\n",
    "    reviews = [comb[0] for comb in combined]\n",
    "    labels = [comb[1] for comb in combined]\n",
    "    return reviews, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews into lists \n",
    "train_val_neg = load_dir_reviews('aclImdb/train/neg')\n",
    "train_val_pos = load_dir_reviews('aclImdb/train/pos')\n",
    "test_neg = load_dir_reviews('aclImdb/test/neg')\n",
    "test_pos = load_dir_reviews('aclImdb/test/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split train into train vs. validation sets \n",
    "train_split = int(20000 / 2) \n",
    "train_neg = train_val_neg[:train_split]\n",
    "train_pos = train_val_pos[:train_split]\n",
    "val_neg = train_val_neg[train_split:]\n",
    "val_pos = train_val_pos[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# combine pos and neg reviews to get unified datasets \n",
    "train_data, train_labels = combine_data(train_neg, train_pos)\n",
    "val_data, val_labels = combine_data(val_neg, val_pos)\n",
    "test_data, test_labels = combine_data(test_neg, test_pos)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews - optimized  \n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation \n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "        a list comprising all the tokens from the dataset \"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "                                               disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = lower_case_remove_punc(sample)\n",
    "        token_dataset.append(tokens)\n",
    "\n",
    "    return token_dataset\n",
    "\n",
    "def save_tokens_to_disk(dataset, destination_path): \n",
    "    \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "    start_time = time.time() \n",
    "    token_dataset = tokenize_dataset(dataset)\n",
    "    with open(destination_path, \"wb\") as f: \n",
    "        pkl.dump(token_dataset, f)\n",
    "    time_elapsed = (time.time() - start_time) / 60.0 \n",
    "    print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(destination_path, time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews - slow \n",
    "\n",
    "# tokenizer = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation \n",
    "\n",
    "# def tokenize(review): \n",
    "#     \"\"\" Takes review as input and outputs a list of tokens in lowercase without punctuation \"\"\" \n",
    "#     tokens = tokenizer(review)\n",
    "#     return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# def tokenize_dataset(dataset):\n",
    "#     \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "#         a list comprising all the tokens from the dataset \"\"\"\n",
    "#     token_dataset = []\n",
    "#     for sample in dataset:\n",
    "#         tokens = tokenize(sample)\n",
    "#         token_dataset.append(tokens)\n",
    "#     return token_dataset \n",
    "\n",
    "# def save_tokens_to_disk(dataset, destination_path): \n",
    "#     \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "#     start_time = time.time() \n",
    "#     token_dataset = tokenize_dataset(dataset)\n",
    "#     with open(destination_path, \"wb\") as f: \n",
    "#         pkl.dump(token_dataset, f)\n",
    "#     time_elapsed = (time.time() - start_time) / 60.0 \n",
    "#     print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(destination_path, time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lower_case_remove_punc(parsed):\n",
    "#     return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "# def tokenize_dataset(dataset):\n",
    "#     token_dataset = []\n",
    "#     for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "#                                                disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "#         tokens = lower_case_remove_punc(sample)\n",
    "#         token_dataset.append(tokens)\n",
    "\n",
    "#     return token_dataset\n",
    "\n",
    "# start_time = time.time() \n",
    "# token_dataset = tokenize_dataset(val_data)\n",
    "# time_elapsed = (time.time() - start_time) / 60.0 \n",
    "# print(time_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89fb55bfaae487eadb85167f43415cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac7c10cd034464484e676d0304fbfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/train_data_tokens.p in 0.9 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d05b01ed1a42b78c6bb13029a008d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/test_data_tokens.p in 0.9 minutes\n"
     ]
    }
   ],
   "source": [
    "save_tokens_to_disk(val_data, \"data/val_data_tokens.p\")\n",
    "save_tokens_to_disk(train_data, \"data/train_data_tokens.p\")\n",
    "save_tokens_to_disk(test_data, \"data/test_data_tokens.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4808696\n"
     ]
    }
   ],
   "source": [
    "# load saved tokens \n",
    "train_data_tokens = pkl.load(open(\"data/train_data_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"data/val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"data/test_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "\n",
    "# double check \n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261965, 130352, 128951, 116768, 107941, 87781, 74745, 74062, 66302, 58666)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter = Counter(all_train_tokens) \n",
    "vocab, count = zip(*token_counter.most_common(10))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from 10000 most common tokens in the training set \n",
    "\n",
    "max_vocab_size = 10000 \n",
    "PAD_IDX = 0 \n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens): \n",
    "    \"\"\" Takes list of all tokens and returns:\n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token \n",
    "    \n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 7162 ; token weaker\n",
      "Token weaker; token id 7162\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset \n",
    "\n",
    "def token2index_dataset(tokens_data): \n",
    "    indices_data = []\n",
    "    for datum in tokens_data: \n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in datum]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data \n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# check size of data \n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'were', \"n't\", 'there', 'then', 'unfortunately', 'this', 'movie', 'will', 'be', 'beyond', 'compassion', 'for', 'you', 'which', 'as', 'i', 'say', 'is', 'a', 'shame', 'because', 'although', 'some', 'of', 'the', 'acting', 'is', 'amateurish', 'it', 'is', 'meant', 'to', 'be', 'for', 'realism', 'let', \"'s\", 'face', 'it', '--', 'in', 'real', 'life', 'we', 'do', \"n't\", 'say', 'things', 'in', 'an', 'exacting', 'or', 'perfect', 'way', 'even', 'when', 'we', 'mean', 'to', 'in', 'this', 'sense', 'it', 'works', 'this', 'however', 'does', 'not', 'apply', 'to', 'our', 'known', 'actors', 'in', 'this', 'film', 'notably', 'jodie', 'foster', 'born', 'a', 'natural', 'the', 'fact', 'that', 'the', 'other', '3', 'girls', 'are', 'not', 'accomplished', 'only', 'adds', 'to', 'the', 'story', '--', 'jodie', 'plays', 'the', 'glue', 'that', 'struggles', 'to', 'keep', 'their', 'friendship', 'close', 'even', 'with', 'the', 'obvious', 'feeling', 'of', 'fatality', 'meaning', 'that', 'no', 'matter', 'how', 'close', 'friends', 'are', 'eventually', 'there', 'are', 'some', 'people', 'that', 'just', 'fade', 'away', 'no', 'matter', 'how', 'you', 'try.<br', '/><br', '/>and', 'therein', 'is', 'the', 'core', 'of', 'the', 'movie', 'it', \"'s\", 'not', 'about', 'partying', 'it', \"'s\", 'not', 'about', 'sexuality', 'but', 'about', 'these', '4', 'girls', 'and', 'their', 'final', 'time', 'as', 'still', 'young', 'girls', 'before', 'they', 'have', 'to', 'go', 'the', 'world', 'alone.<br', '/><br', '/>if', 'you', 'have', 'ever', 'had', 'a', 'friendship', 'like', 'that', 'in', 'your', 'life', 'you', 'will', 'feel', 'this', 'movie', '--', 'it', 'will', 'mean', 'a', 'lot', 'to', 'you', 'no', 'matter', 'what', 'era', 'it', 'is', 'set', 'in', 'or', 'what', 'era', 'you', 'grew', 'up', 'in', 'we', 'all', 'knew', 'these', 'girls', 'in', 'school', 'or', 'at', 'the', 'very', 'least', 'knew', 'of', 'them', 'we', 'all', 'knew', 'the', 'frustrated', 'virgin', 'half', 'wanting', 'to', 'hold', 'onto', 'childhood', 'and', 'half', 'wanting', 'desperately', 'to', 'grow', 'up', 'and', 'thinking', 'that', 'will', 'do', 'it', 'for', 'her', 'we', 'all', 'knew', 'the', 'boy', 'crazy', 'one', 'the', 'fashion', 'plate', 'whose', 'vanity', 'hides', 'her', 'fear', 'of', 'the', 'world', 'her', 'fear', 'of', 'acceptance', 'we', 'all', 'knew', 'the', 'party', 'girl', 'the', 'one', 'they', 'whispered', 'about', 'with', 'tales', 'of', 'not', 'only', 'her', 'sad', 'home', 'life', 'but', 'of', 'her', 'notorious', 'exploits', 'and', 'we', 'all', 'knew', 'the', 'mother', 'figure', 'the', 'one', 'a', 'little', 'more', 'real', 'a', 'little', 'more', 'grounded', 'a', 'little', 'more', 'sad', 'because', 'she', 'knew', 'what', 'would', 'happen', 'maybe', 'you', 'were', 'one', 'of', 'those', 'girls', 'maybe', 'like', 'me', 'you', 'had', 'been', 'each', 'one', 'at', 'one', 'time', 'or', 'another', '...', '<br', '/><br', '/>this', 'film', 'really', 'captures', 'that', 'fragile', 'time', 'in', 'life', 'when', 'want', 'needs', 'pressures', 'womanhood', 'childhood', 'the', 'world', 'and', 'loneliness', 'are', 'all', 'embodied', 'in', 'each', 'female', \"'s\", 'head', 'each', 'factor', 'on', 'the', 'precipice', 'which', 'aspect', 'do', 'you', 'hang', 'on', 'to', 'what', 'do', 'you', 'toss', 'over', 'the', 'edge', 'no', 'matter', 'how', 'you', 'may', 'want', 'to', 'hold', 'on', 'and', 'how', 'painful', 'is', 'goodbye', 'to', 'everything', 'you', \"'ve\", 'known', 'that', \"'s\", 'what', 'this', 'movie', 'is', '--', 'steps', 'into', 'womanhood', 'while', 'clinging', 'onto', 'childhood', 'and', 'how', 'damn', 'tough', 'it', 'is', 'to', 'keep', 'walking', 'if', 'you', 'were', 'there', 'you', 'know', '...', 'and', 'love', 'this', 'film', 'as', 'i', 'do', 'aching', 'and', 'tenderly', 'done', 'a', 'fine', 'piece', 'of', 'captured', 'femininity']\n",
      "[49, 23, 70, 24, 44, 99, 507, 11, 19, 83, 30, 727, 4928, 17, 23, 64, 16, 10, 135, 7, 4, 946, 85, 270, 51, 5, 2, 122, 7, 2367, 8, 7, 907, 6, 30, 17, 1947, 284, 13, 394, 8, 172, 9, 149, 119, 72, 47, 24, 135, 193, 9, 36, 1, 42, 393, 98, 61, 56, 72, 386, 6, 9, 11, 285, 8, 476, 11, 206, 75, 25, 6577, 6, 261, 546, 153, 9, 11, 21, 3906, 5769, 2383, 1425, 4, 1175, 2, 189, 12, 2, 87, 471, 567, 26, 25, 3840, 65, 1581, 6, 2, 66, 172, 5769, 296, 2, 1, 12, 3056, 6, 391, 71, 2139, 478, 61, 18, 2, 573, 538, 5, 1, 1244, 12, 59, 553, 93, 478, 369, 26, 869, 44, 26, 51, 84, 12, 43, 5414, 244, 59, 553, 93, 23, 1, 14, 807, 1, 7, 2, 2016, 5, 2, 19, 8, 13, 25, 45, 1, 8, 13, 25, 45, 3003, 20, 45, 136, 725, 567, 3, 71, 481, 60, 16, 131, 184, 567, 159, 34, 29, 6, 141, 2, 179, 9259, 14, 642, 23, 29, 127, 68, 4, 2139, 39, 12, 9, 130, 119, 23, 83, 233, 11, 19, 172, 8, 83, 386, 4, 173, 6, 23, 59, 553, 50, 992, 8, 7, 264, 9, 42, 50, 992, 23, 2023, 58, 9, 72, 32, 677, 136, 567, 9, 389, 42, 33, 2, 55, 218, 677, 5, 102, 72, 32, 677, 2, 3503, 2937, 326, 1733, 6, 1062, 1592, 1497, 3, 326, 1733, 2707, 6, 2172, 58, 3, 532, 12, 83, 47, 8, 17, 41, 72, 32, 677, 2, 423, 920, 31, 2, 1636, 7831, 596, 6810, 5930, 41, 1108, 5, 2, 179, 41, 1108, 5, 5770, 72, 32, 677, 2, 1089, 240, 2, 31, 34, 1, 45, 18, 2870, 5, 25, 65, 41, 667, 348, 119, 20, 5, 41, 2619, 6370, 3, 72, 32, 677, 2, 415, 787, 2, 31, 4, 123, 54, 149, 4, 123, 54, 9720, 4, 123, 54, 667, 85, 53, 677, 50, 57, 616, 286, 23, 70, 31, 5, 148, 567, 286, 39, 74, 23, 68, 81, 254, 31, 33, 31, 60, 42, 171, 79, 493, 14, 290, 21, 67, 2349, 12, 6924, 60, 9, 119, 56, 182, 716, 1, 1, 1497, 2, 179, 3, 5486, 26, 32, 1, 9, 254, 645, 13, 416, 254, 2479, 22, 2, 1, 64, 1248, 47, 23, 3323, 22, 6, 50, 47, 23, 8847, 125, 2, 1280, 59, 553, 93, 23, 196, 182, 6, 1062, 22, 3, 93, 1355, 7, 7158, 6, 283, 23, 146, 546, 12, 13, 50, 11, 19, 7, 172, 3177, 88, 1, 143, 1, 1592, 1497, 3, 93, 1576, 1170, 8, 7, 6, 391, 1306, 49, 23, 70, 44, 23, 126, 79, 3, 120, 11, 21, 16, 10, 47, 1, 3, 1, 221, 4, 466, 417, 5, 2017, 1]\n"
     ]
    }
   ],
   "source": [
    "# check tokenization of dataset \n",
    "print(train_data_tokens[0])\n",
    "print(train_data_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class and collate function for data loader \n",
    "\n",
    "class MovieReviewsDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of movie review tokens and a list of labels \n",
    "        \"\"\"\n",
    "        self.data_list = data_list \n",
    "        self.label_list = label_list \n",
    "        assert (len(self.data_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, length of list, and label of the data point\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "def collate_func(batch): \n",
    "    \"\"\" \n",
    "    Customized function for DataLoader that dynamically pads the batch so that the data have the same length\n",
    "    \"\"\"\n",
    "    data_list = [] \n",
    "    label_list = [] \n",
    "    length_list = [] \n",
    "    \n",
    "    for datum in batch:         \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        # pad data before appending \n",
    "        padded_vec = np.pad(array = np.array(datum[0]), \n",
    "                            pad_width = ((0, MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                            mode = 'constant', \n",
    "                            constant_values = 0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders for train/val/test datasets \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = MovieReviewsDataset(train_data_indices, train_labels) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n",
    "                                           collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewsDataset(val_data_indices, val_labels) \n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, \n",
    "                                         collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewsDataset(test_data_indices, test_labels) \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, \n",
    "                                          collate_fn = collate_func, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a sample datum (of data length 200):\n",
      "Data is: tensor([  10,  200,   11,  683,   48,    5,  305,   32,  683, 3682,   26,   17,\n",
      "           1,   37, 1620,    4,   55,  525,  264,    5,   28, 1535,  249,  817,\n",
      "         507,    2,  496,    5,    2,  373,  160,    3,  793,    7,    1,   22,\n",
      "           2, 3740,   72,   29, 3351,  354,  394,  659,    5,    1,  124,   27,\n",
      "          80,  220,   30,  399,  232,  633,    4, 7553,  383,   72,   47,   24,\n",
      "          69,   28,  967,   22,    2, 5700,   12,  937,    2,  215,   90,   72,\n",
      "         187,  345,  930,  659,    5,    1,   38,  477,   44,  733,    6,   30,\n",
      "         289, 4000,   22,    1,   20, 2318, 7223, 8884,   32,    5,  102,    2,\n",
      "         227,    9, 1042,    5,    1,  188,    6,   30, 6264,    6, 6728,   28,\n",
      "        1134,    1,   45,    2,  825,   16,   49,    1,   22,    2, 5700,    7,\n",
      "           4, 2576, 7454,  143,    2,  227,  477,    7,    1,   22,    1,    9,\n",
      "          22,    4,  166, 4719,    9,    2,    1,   77,  167,    1,    2, 1056,\n",
      "        5700,  160,   12,  937, 8581,   48,    2,    1,    2,  464, 4782,    7,\n",
      "          52, 1247,    6,    1,    1,    1,   22,    2, 4412,   26, 3064,    5,\n",
      "        1431, 5042,    1,  425, 2344,  550,   22, 4412,   39, 1877, 1044,  592,\n",
      "         428,    5,   21,   49,    1,   68, 1499,   61,  171,  111,  154, 5912,\n",
      "           7,   31,    5,  148,  193,   12,   57,  117])\n",
      "Label is 0\n"
     ]
    }
   ],
   "source": [
    "# test data loader on training data \n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (\"Printing a sample datum (of data length {}):\".format(lengths[0]))\n",
    "    print (\"Data is: {}\".format((data[0])))\n",
    "    print (\"Label is {}\".format((labels[0])))\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-Of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture and helper methods \n",
    "\n",
    "class BagOfWords(nn.Module): \n",
    "    \"\"\" \n",
    "    BagOfWords classification model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim): \n",
    "        \"\"\" \n",
    "        @param vocab_size: size of the vocabulary \n",
    "        @param emd_dim: size of the word embedding \n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "        \n",
    "    def forward(self, data, length): \n",
    "        \"\"\" \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a review\n",
    "            that is represented using n-gram index. Note that they are padded to have the same length. \n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (i.e. non-padded)\n",
    "            length of each sentence in the data \n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "        \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "\n",
    "        return out\n",
    "    \n",
    "def test_model(loader, model): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    model.eval() \n",
    "    \n",
    "    for data_batch, length_batch, label_batch in loader: \n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predictions = outputs.max(1, keepdim=True)[1]    \n",
    "        total += label_batch.size(0)\n",
    "        correct += predictions.eq(label_batch.view_as(predictions)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and hyperparameters \n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 2\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [100/625], Total Steps: 100, Validation Acc: 80.04, Train Acc: 82.255\n",
      "Epoch: [1/2], Step: [200/625], Total Steps: 200, Validation Acc: 83.3, Train Acc: 87.06\n",
      "Epoch: [1/2], Step: [300/625], Total Steps: 300, Validation Acc: 83.86, Train Acc: 88.745\n",
      "Epoch: [1/2], Step: [400/625], Total Steps: 400, Validation Acc: 84.1, Train Acc: 90.835\n",
      "Epoch: [1/2], Step: [500/625], Total Steps: 500, Validation Acc: 84.34, Train Acc: 91.795\n",
      "Epoch: [1/2], Step: [600/625], Total Steps: 600, Validation Acc: 84.4, Train Acc: 92.375\n",
      "Epoch: [2/2], Step: [100/625], Total Steps: 725, Validation Acc: 83.64, Train Acc: 93.295\n",
      "Epoch: [2/2], Step: [200/625], Total Steps: 825, Validation Acc: 83.2, Train Acc: 93.47\n",
      "Epoch: [2/2], Step: [300/625], Total Steps: 925, Validation Acc: 83.12, Train Acc: 94.005\n",
      "Epoch: [2/2], Step: [400/625], Total Steps: 1025, Validation Acc: 83.28, Train Acc: 94.485\n",
      "Epoch: [2/2], Step: [500/625], Total Steps: 1125, Validation Acc: 83.76, Train Acc: 94.99\n",
      "Epoch: [2/2], Step: [600/625], Total Steps: 1225, Validation Acc: 82.78, Train Acc: 95.58\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate on Validation Set \n",
    "\n",
    "total_steps = 0 \n",
    "results = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and (i+1) % 100 == 0:\n",
    "            total_steps = epoch * 625 + (i+1) \n",
    "            train_acc = test_model(train_loader, model) # report train accuracy \n",
    "            val_acc = test_model(val_loader, model) # report validation accuracy \n",
    "            result = {} \n",
    "            result['step'] = total_steps \n",
    "            result['train_acc'] = train_acc\n",
    "            result['val_acc'] = val_acc        \n",
    "            results.append(result)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Total Steps: {}, Validation Acc: {}, Train Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), total_steps, val_acc, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x133bc4f28>"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curve \n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df = results_df.set_index('step')\n",
    "results_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Preprocessing and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
