{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time \n",
    "from os import listdir \n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to load reviews from directories \n",
    "\n",
    "def load_single_review(fdir, fname): \n",
    "    \"\"\" Takes as input file directory and file name of a single review, returns review as string \"\"\"\n",
    "    fpath = fdir + '/' + fname \n",
    "    with open(fpath, 'r') as f: \n",
    "        review = f.read()\n",
    "        return review \n",
    "    \n",
    "def load_dir_reviews(fdir): \n",
    "    \"\"\" Takes as input file directory where reviews are stored, returns them as a list of review strings \"\"\"\n",
    "    fnames = [f for f in listdir(fdir)]\n",
    "    reviews = [load_single_review(fdir, fname) for fname in fnames]\n",
    "    return reviews\n",
    "\n",
    "def combine_data(neg_reviews, pos_reviews): \n",
    "    \"\"\" Combines lists of negative and positive reviews, returns a combined dataset comprising reviews and labels \"\"\"\n",
    "    neg_with_labels = [(review, 0) for review in neg_reviews] \n",
    "    pos_with_labels = [(review, 1) for review in pos_reviews]\n",
    "    combined = neg_with_labels + pos_with_labels\n",
    "    combined = random.sample(combined, len(combined))\n",
    "    reviews = [comb[0] for comb in combined]\n",
    "    labels = [comb[1] for comb in combined]\n",
    "    return reviews, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews into lists \n",
    "train_val_neg = load_dir_reviews('aclImdb/train/neg')\n",
    "train_val_pos = load_dir_reviews('aclImdb/train/pos')\n",
    "test_neg = load_dir_reviews('aclImdb/test/neg')\n",
    "test_pos = load_dir_reviews('aclImdb/test/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split train into train vs. validation sets \n",
    "train_split = int(20000 / 2) \n",
    "train_neg = train_val_neg[:train_split]\n",
    "train_pos = train_val_pos[:train_split]\n",
    "val_neg = train_val_neg[train_split:]\n",
    "val_pos = train_val_pos[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# combine pos and neg reviews to get unified datasets \n",
    "train_data, train_labels = combine_data(train_neg, train_pos)\n",
    "val_data, val_labels = combine_data(val_neg, val_pos)\n",
    "test_data, test_labels = combine_data(test_neg, test_pos)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews \n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation \n",
    "\n",
    "def tokenize(review): \n",
    "    \"\"\" Takes review as input and outputs a list of tokens in lowercase without punctuation \"\"\" \n",
    "    tokens = tokenizer(review)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "        a list comprising all the tokens from the dataset \"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "    return token_dataset \n",
    "\n",
    "def save_tokens_to_disk(dataset, destination_path): \n",
    "    \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "    start_time = time.time() \n",
    "    token_dataset = tokenize_dataset(dataset)\n",
    "    with open(destination_path, \"wb\") as f: \n",
    "        pkl.dump(token_dataset, f)\n",
    "    time_elapsed = (time.time() - start_time) / 60.0 \n",
    "    print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(destination_path, time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tokenized and saved as data/val_data_tokens.p in 4.9 minutes\n",
      "Data tokenized and saved as data/train_data_tokens.p in 15.6 minutes\n",
      "Data tokenized and saved as data/test_data_tokens.p in 16.7 minutes\n"
     ]
    }
   ],
   "source": [
    "save_tokens_to_disk(val_data, \"data/val_data_tokens.p\")\n",
    "save_tokens_to_disk(train_data, \"data/train_data_tokens.p\")\n",
    "save_tokens_to_disk(test_data, \"data/test_data_tokens.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 4808696\n"
     ]
    }
   ],
   "source": [
    "# load saved tokens \n",
    "train_data_tokens = pkl.load(open(\"data/train_data_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"data/val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"data/test_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "\n",
    "# double check \n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261965, 130352, 128951, 116768, 107941, 87781, 74745, 74062, 66302, 58666)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter = Counter(all_train_tokens) \n",
    "vocab, count = zip(*token_counter.most_common(10))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from 10000 most common tokens in the training set \n",
    "\n",
    "max_vocab_size = 10000 \n",
    "PAD_IDX = 0 \n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens): \n",
    "    \"\"\" Takes list of all tokens and returns:\n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token \n",
    "    \n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 5716 ; token receiving\n",
      "Token receiving; token id 5716\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset \n",
    "\n",
    "def token2index_dataset(tokens_data): \n",
    "    indices_data = []\n",
    "    for datum in tokens_data: \n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in datum]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data \n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# check size of data \n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'was', 'lying', 'on', 'my', 'bed', 'with', 'a', 'really', 'bad', 'cold', 'or', 'flu', 'or', 'whatever', 'i', 'figure', 'maybe', 'i', \"'d\", 'kill', 'some', 'time', 'watching', 'some', 'horror', 'movies', 'my', 'mom', 'bought', 'for', 'me', 'a', 'little', 'while', 'ago', 'i', 'wish', 'i', 'never', 'picked', 'this', 'movie', 'after', 'i', 'watched', 'it', 'i', 'felt', 'even', 'more', 'sick', 'and', 'i', 'wanted', 'to', 'throw', 'up', 'afterwords(when', 'i', 'got', 'better', 'of', 'course', 'i', 'did', 'some', 'research', 'on', 'dennis', 'l.rader', 'and', 'i', 'noticed', 'that', 'the', 'dennis', 'in', 'the', 'movie', 'was', 'nothing', 'like', 'the', 'real', 'one', 'i', 'hope', 'that', 'no', 'one', 'ever', 'watches', 'this', 'movie', 'but', 'if', 'they', 'ever', 'do', 'do', \"n't\", 'eat', 'or', 'you', \"'ll\", 'feel', 'the', 'way', 'i', 'felt', 'after', 'i', 'first', 'watched', 'it', 'i', 'think', 'you', 'would', 'have', 'a', 'better', 'time', 'watching', 'the', 'santa', 'claus', '3', 'at', 'least', 'that', 'movie', 'had', 'better', 'reviews', 'on', 'this', 'site']\n",
      "[10, 15, 3092, 22, 63, 1396, 18, 4, 67, 89, 1015, 42, 1, 42, 861, 10, 787, 286, 10, 277, 498, 51, 60, 151, 51, 190, 104, 63, 1479, 1209, 17, 74, 4, 123, 143, 598, 10, 632, 10, 117, 1636, 11, 19, 105, 10, 291, 8, 10, 413, 61, 54, 1163, 3, 10, 457, 6, 1361, 58, 1, 10, 183, 132, 5, 256, 10, 73, 51, 2292, 22, 3339, 1, 3, 10, 1913, 12, 2, 3339, 9, 2, 19, 15, 164, 39, 2, 149, 31, 10, 435, 12, 59, 31, 127, 3575, 11, 19, 20, 49, 34, 127, 47, 47, 24, 2182, 42, 23, 245, 233, 2, 98, 10, 413, 105, 10, 91, 291, 8, 10, 107, 23, 57, 29, 4, 132, 60, 151, 2, 2284, 5930, 471, 33, 218, 12, 19, 68, 132, 859, 22, 11, 2183]\n"
     ]
    }
   ],
   "source": [
    "# check tokenization of dataset \n",
    "print(train_data_tokens[0])\n",
    "print(train_data_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class and collate function for data loader \n",
    "\n",
    "class MovieReviewsDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of movie review tokens and a list of labels \n",
    "        \"\"\"\n",
    "        self.data_list = data_list \n",
    "        self.label_list = label_list \n",
    "        assert (len(self.data_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, length of list, and label of the data point\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "def collate_func(batch): \n",
    "    \"\"\" \n",
    "    Customized function for DataLoader that dynamically pads the batch so that the data have the same length\n",
    "    \"\"\"\n",
    "    data_list = [] \n",
    "    label_list = [] \n",
    "    length_list = [] \n",
    "    \n",
    "    for datum in batch:         \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        # pad data before appending \n",
    "        padded_vec = np.pad(array = np.array(datum[0]), \n",
    "                            pad_width = ((0, MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                            mode = 'constant', \n",
    "                            constant_values = 0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders for train/val/test datasets \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200 \n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "train_dataset = MovieReviewsDataset(train_data_indices, train_labels) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n",
    "                                           collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewsDataset(val_data_indices, val_labels) \n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, \n",
    "                                         collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewsDataset(test_data_indices, test_labels) \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, \n",
    "                                          collate_fn = collate_func, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a sample datum (of data length 200):\n",
      "Data is: tensor([   2, 2171,   37,  430,   11,   19,    4, 8224,   46,  687, 1038,    2,\n",
      "         215,    8,   13,  573,   27,   46,   24,   81,  184,   42, 1315,    9,\n",
      "           4,   55,  195,   60,   42,   27,  231,   29, 2710,   12,    2, 1670,\n",
      "           5,    8,   15,   12,    2,   76, 1246,  184,  108,  167,  198,   34,\n",
      "          80,   97,    4, 1548,   35, 1455,   58, 5509,    3, 2278,    4, 6865,\n",
      "          17, 2731,   49,   65,    8,   15,   12,  782,   20,    2,    1,  852,\n",
      "           3,    1,   33,   84,   37,    1,  359,   71,  121,    6,   97,  193,\n",
      "         132,   16,    2,  926,  214,  455,    3,  455,  174,  637,  249,   76,\n",
      "          49,   23,  187,   25,  175,    5,    2, 5104,   23,  187,  175,    5,\n",
      "           2,    1,   14,  106, 1325,  768,    7,   12, 1214,    1,   54, 1214,\n",
      "          64,    1,   61,   54,    9,   36,  127,    1, 7650,  201, 3139,   83,\n",
      "         362,   23, 2889,    5,    1, 2580,   35,    2,   87,  487,   64,   34,\n",
      "         107, 4258,   71, 7789,   61,   54,    9,    1,  124,   75,    8,  137,\n",
      "           3,  701,   27, 1038,    2, 5034,    5,    2, 7129, 5706,    6,    9,\n",
      "           2,  399,   64,   15,   12,   84,  586,    9, 9025, 9932,   26, 3093,\n",
      "         606,  128,   38,    2, 1509, 6128,    5,   50,    7,  168,   22,   32,\n",
      "         192,  102,   64,    7, 3744,  140,   34,  459])\n",
      "Label is 1\n"
     ]
    }
   ],
   "source": [
    "# test data loader on training data \n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (\"Printing a sample datum (of data length {}):\".format(lengths[0]))\n",
    "    print (\"Data is: {}\".format((data[0])))\n",
    "    print (\"Label is {}\".format((labels[0])))\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
