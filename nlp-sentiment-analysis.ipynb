{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import pickle as pkl\n",
    "import time \n",
    "from os import listdir \n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to load reviews from directories \n",
    "\n",
    "def load_single_review(fdir, fname): \n",
    "    \"\"\" Takes as input file directory and file name of a single review, returns review as string \"\"\"\n",
    "    fpath = fdir + '/' + fname \n",
    "    with open(fpath, 'r') as f: \n",
    "        review = f.read()\n",
    "        return review \n",
    "    \n",
    "def load_dir_reviews(fdir): \n",
    "    \"\"\" Takes as input file directory where reviews are stored, returns them as a list of review strings \"\"\"\n",
    "    fnames = [f for f in listdir(fdir)]\n",
    "    reviews = [load_single_review(fdir, fname) for fname in fnames]\n",
    "    return reviews\n",
    "\n",
    "def combine_data(neg_reviews, pos_reviews): \n",
    "    \"\"\" Combines lists of negative and positive reviews, returns a combined dataset comprising reviews and labels \"\"\"\n",
    "    neg_with_labels = [(review, 0) for review in neg_reviews] \n",
    "    pos_with_labels = [(review, 1) for review in pos_reviews]\n",
    "    combined = neg_with_labels + pos_with_labels\n",
    "    combined = random.sample(combined, len(combined))\n",
    "    reviews = [comb[0] for comb in combined]\n",
    "    labels = [comb[1] for comb in combined]\n",
    "    return reviews, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reviews into lists \n",
    "train_val_neg = load_dir_reviews('aclImdb/train/neg')\n",
    "train_val_pos = load_dir_reviews('aclImdb/train/pos')\n",
    "test_neg = load_dir_reviews('aclImdb/test/neg')\n",
    "test_pos = load_dir_reviews('aclImdb/test/pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split train into train vs. validation sets \n",
    "train_split = int(20000 / 2) \n",
    "train_neg = train_val_neg[:train_split]\n",
    "train_pos = train_val_pos[:train_split]\n",
    "val_neg = train_val_neg[train_split:]\n",
    "val_pos = train_val_pos[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Validation dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# combine pos and neg reviews to get unified datasets \n",
    "train_data, train_labels = combine_data(train_neg, train_pos)\n",
    "val_data, val_labels = combine_data(val_neg, val_pos)\n",
    "test_data, test_labels = combine_data(test_neg, test_pos)\n",
    "print (\"Train dataset size is {}\".format(len(train_data)))\n",
    "print (\"Validation dataset size is {}\".format(len(val_data)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to tokenize reviews - optimized  \n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation \n",
    "spacy_stop_words = tokenizer.Defaults.stop_words\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def lower_case_remove_punc_nltk_stopwords_lemmatize(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.lemma_.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.lemma_.lower() not in nltk_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc_nltk_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in nltk_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc_spacy_stopwords(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed \n",
    "            if (token.text not in punctuations and token.text.lower() not in spacy_stop_words)]\n",
    "\n",
    "def lower_case_remove_punc(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed if (token.text not in punctuations)]\n",
    "\n",
    "def lower_case(parsed):\n",
    "    \"\"\" Takes text as input and outputs a list of tokens in lowercase without punctuation \"\"\"\n",
    "    return [token.text.lower() for token in parsed]\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    \"\"\" Takes as input a dataset comprising a list of reviews, outputs the tokenized dataset along with \n",
    "        a list comprising all the tokens from the dataset \"\"\"\n",
    "    token_dataset = []\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, \n",
    "                                               disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "#        tokens = lower_case_remove_punc_nltk_stopwords_lemmatize(sample)\n",
    "#        tokens = lower_case_remove_punc_nltk_stopwords(sample)\n",
    "#        tokens = lower_case_remove_punc_spacy_stopwords(sample)\n",
    "#        tokens = lower_case_remove_punc(sample)\n",
    "        tokens = lower_case(sample)\n",
    "        token_dataset.append(tokens)\n",
    "\n",
    "    return token_dataset\n",
    "\n",
    "def save_tokens_to_disk(dataset, destination_path): \n",
    "    \"\"\" Tokenize dataset as save as pickle to destination path \"\"\"\n",
    "    start_time = time.time() \n",
    "    token_dataset = tokenize_dataset(dataset)\n",
    "    with open(destination_path, \"wb\") as f: \n",
    "        pkl.dump(token_dataset, f)\n",
    "    time_elapsed = (time.time() - start_time) / 60.0 \n",
    "    print(\"Data tokenized and saved as {} in {:.1f} minutes\".format(destination_path, time_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fadc672e3b4b78a2af77524287c89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/val_data_tokens.p in 0.2 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e85e742268b48bcac9a2357958b333c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/train_data_tokens.p in 0.7 minutes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dad54c89b54c31a5e88149435205c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data tokenized and saved as data/test_data_tokens.p in 0.7 minutes\n"
     ]
    }
   ],
   "source": [
    "save_tokens_to_disk(val_data, \"data/val_data_tokens.p\")\n",
    "save_tokens_to_disk(train_data, \"data/train_data_tokens.p\")\n",
    "save_tokens_to_disk(test_data, \"data/test_data_tokens.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 5439615\n"
     ]
    }
   ],
   "source": [
    "# load saved tokens \n",
    "train_data_tokens = pkl.load(open(\"data/train_data_tokens.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"data/val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"data/test_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = [item for sublist in train_data_tokens for item in sublist] \n",
    "\n",
    "# double check \n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(261965, 219805, 188590, 130352, 128951, 116768, 107941, 87781, 74745, 74062)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter = Counter(all_train_tokens) \n",
    "vocab, count = zip(*token_counter.most_common(10))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from 10000 most common tokens in the training set \n",
    "\n",
    "PAD_IDX = 0 \n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens, max_vocab_size=10000): \n",
    "    \"\"\" Takes list of all tokens and returns:\n",
    "        - id2token: list of tokens, where id2token[i] returns token that corresponds to i-th token \n",
    "        - token2id: dictionary where keys represent tokens and corresponding values represent their indices\n",
    "    \"\"\"\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2, 2+len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token \n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token \n",
    "    \n",
    "token2id, id2token = build_vocab(all_train_tokens, max_vocab_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 4594 ; token swim\n",
      "Token swim; token id 4594\n"
     ]
    }
   ],
   "source": [
    "# check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset \n",
    "\n",
    "def token2index_dataset(tokens_data): \n",
    "    indices_data = []\n",
    "    for datum in tokens_data: \n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in datum]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data \n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# check size of data \n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'you', 'were', \"n't\", 'there', ',', 'then', 'unfortunately', 'this', 'movie', 'will', 'be', 'beyond', 'compassion', 'for', 'you', '.', 'which', 'as', 'i', 'say', 'is', 'a', 'shame', 'because', 'although', 'some', 'of', 'the', 'acting', 'is', 'amateurish', ',', 'it', 'is', 'meant', 'to', 'be', 'for', 'realism', '.', 'let', \"'s\", 'face', 'it', '--', 'in', 'real', 'life', ',', 'we', 'do', \"n't\", 'say', 'things', 'in', 'an', 'exacting', 'or', 'perfect', 'way', ',', 'even', 'when', 'we', 'mean', 'to', '.', 'in', 'this', 'sense', ',', 'it', 'works', '.', 'this', ',', 'however', ',', 'does', 'not', 'apply', 'to', 'our', '\"', 'known', '\"', 'actors', 'in', 'this', 'film', ',', 'notably', 'jodie', 'foster', '(', 'born', 'a', 'natural', ')', '.', 'the', 'fact', 'that', 'the', 'other', '3', 'girls', 'are', 'not', 'accomplished', 'only', 'adds', 'to', 'the', 'story', '--', 'jodie', 'plays', 'the', 'glue', 'that', 'struggles', 'to', 'keep', 'their', 'friendship', 'close', ',', 'even', 'with', 'the', 'obvious', 'feeling', 'of', 'fatality', '.', 'meaning', 'that', 'no', 'matter', 'how', 'close', 'friends', 'are', ',', 'eventually', 'there', 'are', 'some', 'people', 'that', 'just', 'fade', 'away', ',', 'no', 'matter', 'how', 'you', 'try.<br', '/><br', '/>and', 'therein', 'is', 'the', 'core', 'of', 'the', 'movie', '.', 'it', \"'s\", 'not', 'about', 'partying', ',', 'it', \"'s\", 'not', 'about', 'sexuality', ',', 'but', 'about', 'these', '4', 'girls', 'and', 'their', 'final', 'time', 'as', 'still', 'young', 'girls', 'before', 'they', 'have', 'to', 'go', 'the', 'world', 'alone.<br', '/><br', '/>if', 'you', 'have', 'ever', 'had', 'a', 'friendship', 'like', 'that', 'in', 'your', 'life', ',', 'you', 'will', 'feel', 'this', 'movie', '--', 'it', 'will', 'mean', 'a', 'lot', 'to', 'you', ',', 'no', 'matter', 'what', 'era', 'it', 'is', 'set', 'in', ',', 'or', 'what', 'era', 'you', 'grew', 'up', 'in', '.', 'we', 'all', 'knew', 'these', 'girls', 'in', 'school', ',', 'or', 'at', 'the', 'very', 'least', 'knew', 'of', 'them', '.', 'we', 'all', 'knew', 'the', 'frustrated', 'virgin', ',', 'half', 'wanting', 'to', 'hold', 'onto', 'childhood', 'and', 'half', 'wanting', 'desperately', 'to', 'grow', 'up', 'and', 'thinking', 'that', 'will', 'do', 'it', 'for', 'her', '.', 'we', 'all', 'knew', 'the', 'boy', '-', 'crazy', 'one', ',', 'the', 'fashion', 'plate', 'whose', 'vanity', 'hides', 'her', 'fear', 'of', 'the', 'world', ',', 'her', 'fear', 'of', 'acceptance', '.', 'we', 'all', 'knew', 'the', 'party', 'girl', ',', 'the', 'one', 'they', 'whispered', 'about', ',', 'with', 'tales', 'of', 'not', 'only', 'her', 'sad', 'home', 'life', 'but', 'of', 'her', 'notorious', 'exploits', '.', 'and', 'we', 'all', 'knew', 'the', '\"', 'mother', 'figure', '\"', ',', 'the', 'one', 'a', 'little', 'more', 'real', ',', 'a', 'little', 'more', 'grounded', ',', 'a', 'little', 'more', 'sad', 'because', 'she', 'knew', 'what', 'would', 'happen', '.', 'maybe', 'you', 'were', 'one', 'of', 'those', 'girls', '.', 'maybe', ',', 'like', 'me', ',', 'you', 'had', 'been', 'each', 'one', 'at', 'one', 'time', 'or', 'another', '...', '<br', '/><br', '/>this', 'film', 'really', 'captures', 'that', 'fragile', 'time', 'in', 'life', 'when', 'want', ',', 'needs', ',', 'pressures', ',', 'womanhood', ',', 'childhood', ',', 'the', 'world', 'and', 'loneliness', 'are', 'all', 'embodied', 'in', 'each', 'female', \"'s\", 'head', ',', 'each', 'factor', 'on', 'the', 'precipice', '.', 'which', 'aspect', 'do', 'you', 'hang', 'on', 'to', '?', 'what', 'do', 'you', 'toss', 'over', 'the', 'edge', ',', 'no', 'matter', 'how', 'you', 'may', 'want', 'to', 'hold', 'on', '?', 'and', 'how', 'painful', 'is', 'goodbye', 'to', 'everything', 'you', \"'ve\", 'known', '?', 'that', \"'s\", 'what', 'this', 'movie', 'is', '--', 'steps', 'into', 'womanhood', 'while', 'clinging', 'onto', 'childhood', ',', 'and', 'how', 'damn', 'tough', 'it', 'is', 'to', 'keep', 'walking', '.', 'if', 'you', 'were', 'there', ',', 'you', 'know', '...', 'and', 'love', 'this', 'film', ',', 'as', 'i', 'do', '.', 'aching', 'and', 'tenderly', 'done', '.', 'a', 'fine', 'piece', 'of', 'captured', 'femininity', '.']\n",
      "[57, 27, 79, 30, 51, 3, 109, 523, 13, 23, 93, 36, 743, 4952, 21, 27, 4, 73, 20, 12, 148, 9, 6, 964, 95, 285, 59, 7, 2, 134, 9, 2389, 3, 10, 9, 924, 8, 36, 21, 1968, 4, 299, 16, 409, 10, 186, 11, 163, 131, 3, 81, 55, 30, 148, 207, 11, 43, 1, 49, 408, 108, 3, 70, 65, 81, 401, 8, 4, 11, 13, 300, 3, 10, 491, 4, 13, 3, 221, 3, 84, 31, 6605, 8, 276, 15, 562, 15, 167, 11, 13, 25, 3, 3929, 5796, 2405, 28, 1444, 6, 1193, 29, 4, 2, 203, 14, 2, 97, 486, 583, 32, 31, 3863, 74, 1601, 8, 2, 75, 186, 5796, 311, 2, 1, 14, 3079, 8, 406, 80, 2160, 493, 3, 70, 22, 2, 589, 554, 7, 1, 4, 1262, 14, 68, 569, 103, 493, 384, 32, 3, 886, 51, 32, 59, 94, 14, 50, 5441, 259, 3, 68, 569, 103, 27, 1, 18, 823, 1, 9, 2, 2037, 7, 2, 23, 4, 10, 16, 31, 52, 1, 3, 10, 16, 31, 52, 3026, 3, 24, 52, 149, 741, 583, 5, 80, 496, 69, 20, 143, 198, 583, 173, 40, 35, 8, 154, 2, 193, 9287, 18, 658, 27, 35, 139, 77, 6, 2160, 46, 14, 11, 142, 131, 3, 27, 93, 248, 13, 23, 186, 10, 93, 401, 6, 187, 8, 27, 3, 68, 569, 58, 1010, 10, 9, 279, 11, 3, 49, 58, 1010, 27, 2044, 67, 11, 4, 81, 38, 693, 149, 583, 11, 404, 3, 49, 39, 2, 64, 233, 693, 7, 113, 4, 81, 38, 693, 2, 3526, 2960, 3, 341, 1753, 8, 1080, 1612, 1517, 5, 341, 1753, 2729, 8, 2193, 67, 5, 548, 14, 93, 55, 10, 21, 48, 4, 81, 38, 693, 2, 438, 17, 937, 37, 3, 2, 1656, 7859, 612, 6838, 5957, 48, 1126, 7, 2, 193, 3, 48, 1126, 7, 5797, 4, 81, 38, 693, 2, 1107, 255, 3, 2, 37, 40, 1, 52, 3, 22, 2893, 7, 31, 74, 48, 683, 363, 131, 24, 7, 48, 2641, 6398, 4, 5, 81, 38, 693, 2, 15, 430, 803, 15, 3, 2, 37, 6, 135, 63, 163, 3, 6, 135, 63, 9748, 3, 6, 135, 63, 683, 95, 62, 693, 58, 66, 632, 4, 301, 27, 79, 37, 7, 162, 583, 4, 301, 3, 46, 83, 3, 27, 77, 91, 269, 37, 39, 37, 69, 49, 185, 88, 508, 18, 305, 25, 76, 2371, 14, 6952, 69, 11, 131, 65, 196, 3, 732, 3, 1, 3, 1, 3, 1517, 3, 2, 193, 5, 5513, 32, 38, 1, 11, 269, 661, 16, 431, 3, 269, 2501, 26, 2, 1, 4, 73, 1266, 55, 27, 3346, 26, 8, 60, 58, 55, 27, 8875, 137, 2, 1299, 3, 68, 569, 103, 27, 211, 196, 8, 1080, 26, 60, 5, 103, 1374, 9, 7186, 8, 298, 27, 159, 562, 60, 14, 16, 58, 13, 23, 9, 186, 3200, 98, 1, 156, 1, 1612, 1517, 3, 5, 103, 1596, 1188, 10, 9, 8, 406, 1325, 4, 57, 27, 79, 51, 3, 27, 138, 88, 5, 132, 13, 25, 3, 20, 12, 55, 4, 1, 5, 1, 236, 4, 6, 481, 432, 7, 2038, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "# check tokenization of dataset \n",
    "print(train_data_tokens[0])\n",
    "print(train_data_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset class and collate function for data loader \n",
    "\n",
    "class MovieReviewsDataset(Dataset): \n",
    "    \"\"\" \n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, label_list): \n",
    "        \"\"\" \n",
    "        Initialize dataset by passing in a list of movie review tokens and a list of labels \n",
    "        \"\"\"\n",
    "        self.data_list = data_list \n",
    "        self.label_list = label_list \n",
    "        assert (len(self.data_list) == len(self.label_list))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, key): \n",
    "        \"\"\"\n",
    "        Triggered when dataset[i] is called, outputs a list of tokens, length of list, and label of the data point\n",
    "        \"\"\"\n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "    \n",
    "def collate_func(batch): \n",
    "    \"\"\" \n",
    "    Customized function for DataLoader that dynamically pads the batch so that the data have the same length\n",
    "    \"\"\"\n",
    "    data_list = [] \n",
    "    label_list = [] \n",
    "    length_list = [] \n",
    "    \n",
    "    for datum in batch:         \n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "        # pad data before appending \n",
    "        padded_vec = np.pad(array = np.array(datum[0]), \n",
    "                            pad_width = ((0, MAX_SENTENCE_LENGTH - datum[1])), \n",
    "                            mode = 'constant', \n",
    "                            constant_values = 0)\n",
    "        data_list.append(padded_vec)\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders for train/val/test datasets \n",
    "\n",
    "MAX_SENTENCE_LENGTH = 200 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = MovieReviewsDataset(train_data_indices, train_labels) \n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, \n",
    "                                           collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "val_dataset = MovieReviewsDataset(val_data_indices, val_labels) \n",
    "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = BATCH_SIZE, \n",
    "                                         collate_fn = collate_func, shuffle = True)\n",
    "\n",
    "test_dataset = MovieReviewsDataset(test_data_indices, test_labels) \n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, \n",
    "                                          collate_fn = collate_func, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a sample datum (of data length 163):\n",
      "Data is: tensor([1285, 5180,   39,  101,  202,    8,   36,    6,   64, 1495, 1210,   25,\n",
      "           4,    2,  133,  108,   12,  159,   91,  506,    8, 1269,   10,    9,\n",
      "          14,   10,   16,  194,    1,    4,  170,   10,   16,   31,  482, 1495,\n",
      "           4,    2,   75,    9,   52,    1,    3,    6,  198, 1285,  438,    5,\n",
      "          34,  804,    1,    4,    1,    9,   64, 1804,    5, 1496,    3,  221,\n",
      "           3,    1, 1105,   48, 1409,  124,  634,  259,   41,  359,    5,    9,\n",
      "         506,    8, 9131,  341,    7,   10,    4,    2,   75,    9,   52,   80,\n",
      "        2761,    8,  729,    1, 1317,  157,    8, 3251,   18, 6222,    6,  187,\n",
      "           7,    2, 1530,   11,   13,   23,  202,  357, 1495,    3,   10,    9,\n",
      "          31,    4,  102,    7,   10,    9, 4092,   21,  131,    3,  359,    5,\n",
      "           1,    4,   27,   71,  100,   78, 2062,   45,   97, 2893,    3,  152,\n",
      "          20,    1,    5,    1,    4,   13, 3119, 1067,  356,   25,   53,   43,\n",
      "         238,   75,    3, 2974,   22,    6, 2776, 1262,   86,   58,   27,   78,\n",
      "          26,    2, 2673,    7,    2,  282,    4,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Label is 1\n"
     ]
    }
   ],
   "source": [
    "# test data loader on training data \n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (\"Printing a sample datum (of data length {}):\".format(lengths[0]))\n",
    "    print (\"Data is: {}\".format((data[0])))\n",
    "    print (\"Label is {}\".format((labels[0])))\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-Of-Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture and helper methods \n",
    "\n",
    "class BagOfWords(nn.Module): \n",
    "    \"\"\" \n",
    "    BagOfWords classification model \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim): \n",
    "        \"\"\" \n",
    "        @param vocab_size: size of the vocabulary \n",
    "        @param emd_dim: size of the word embedding \n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim, 2)\n",
    "        \n",
    "    def forward(self, data, length): \n",
    "        \"\"\" \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a review\n",
    "            that is represented using n-gram index. Note that they are padded to have the same length. \n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (i.e. non-padded)\n",
    "            length of each sentence in the data \n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "        \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "\n",
    "        return out\n",
    "    \n",
    "def test_model(loader, model): \n",
    "    \"\"\" \n",
    "    Helper function that tests the model's performance on a given dataset \n",
    "    @param: loader = data loader for the dataset to test against \n",
    "    \"\"\"\n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    model.eval() \n",
    "    \n",
    "    for data_batch, length_batch, label_batch in loader: \n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predictions = outputs.max(1, keepdim=True)[1]    \n",
    "        total += label_batch.size(0)\n",
    "        correct += predictions.eq(label_batch.view_as(predictions)).sum().item()\n",
    "        \n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and hyperparameters \n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 2\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [100/625], Total Steps: 100, Validation Acc: 78.22, Train Acc: 80.865\n",
      "Epoch: [1/2], Step: [200/625], Total Steps: 200, Validation Acc: 81.04, Train Acc: 85.76\n",
      "Epoch: [1/2], Step: [300/625], Total Steps: 300, Validation Acc: 83.46, Train Acc: 88.345\n",
      "Epoch: [1/2], Step: [400/625], Total Steps: 400, Validation Acc: 82.62, Train Acc: 90.08\n",
      "Epoch: [1/2], Step: [500/625], Total Steps: 500, Validation Acc: 83.92, Train Acc: 91.055\n",
      "Epoch: [1/2], Step: [600/625], Total Steps: 600, Validation Acc: 83.96, Train Acc: 92.45\n",
      "Epoch: [2/2], Step: [100/625], Total Steps: 725, Validation Acc: 84.08, Train Acc: 93.02\n",
      "Epoch: [2/2], Step: [200/625], Total Steps: 825, Validation Acc: 81.7, Train Acc: 92.33\n",
      "Epoch: [2/2], Step: [300/625], Total Steps: 925, Validation Acc: 82.66, Train Acc: 93.925\n",
      "Epoch: [2/2], Step: [400/625], Total Steps: 1025, Validation Acc: 79.78, Train Acc: 92.285\n",
      "Epoch: [2/2], Step: [500/625], Total Steps: 1125, Validation Acc: 82.74, Train Acc: 94.765\n",
      "Epoch: [2/2], Step: [600/625], Total Steps: 1225, Validation Acc: 81.9, Train Acc: 95.365\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate on Validation Set \n",
    "\n",
    "total_steps = 0 \n",
    "results = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and (i+1) % 100 == 0:\n",
    "            total_steps = epoch * 625 + (i+1) \n",
    "            train_acc = test_model(train_loader, model) # report train accuracy \n",
    "            val_acc = test_model(val_loader, model) # report validation accuracy \n",
    "            result = {} \n",
    "            result['step'] = total_steps \n",
    "            result['train_acc'] = train_acc\n",
    "            result['val_acc'] = val_acc        \n",
    "            results.append(result)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Total Steps: {}, Validation Acc: {}, Train Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), total_steps, val_acc, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1249154e0>"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# learning curve \n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_df = results_df.set_index('step')\n",
    "results_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Preprocessing and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramify(datum_tokens, max_n): \n",
    "    \"\"\" Generates n-grams up to max_n for one given list of tokens representing a datum \"\"\"\n",
    "    result = [] \n",
    "    n = max_n \n",
    "    \n",
    "    # decrement n to append ..., 3-grams, 2-grams to result \n",
    "    while n >= 1: \n",
    "        n_grams = [\" \".join(item) for item in list(zip(*[datum_tokens[i:] for i in range(n)]))]\n",
    "        result = result + n_grams \n",
    "        n = n - 1 \n",
    "        \n",
    "    # when n=1 just append original tokens\n",
    "    result = result + datum_tokens  \n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_dataset(train_tokens, val_tokens, test_tokens, max_n, max_vocab_size):\n",
    "    \"\"\" Takes as input: orignal 1-gram train/val/test tokenized datasets, max_n (for n-gram), max_vocab_size, \n",
    "        and returns: \n",
    "        - token2id, id2token \n",
    "        - train_data_indices, val_data_indices, test_data_indices\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate ngram tokens \n",
    "    train_ngram_tokens = [ngramify(datum, 3) for datum in train_tokens]  \n",
    "    val_ngram_tokens = [ngramify(datum, 3) for datum in val_tokens]  \n",
    "    test_ngram_tokens = [ngramify(datum, 3) for datum in test_tokens]  \n",
    "    all_train_ngram_tokens = [item for sublist in train_ngram_tokens for item in sublist] \n",
    "    \n",
    "    # build vocab \n",
    "    token2id, id2token = build_vocab(all_train_ngram_tokens, max_vocab_size)\n",
    "    \n",
    "    # convert tokens to indices \n",
    "    train_data_indices = token2index_dataset(train_ngram_tokens)\n",
    "    val_data_indices = token2index_dataset(val_ngram_tokens)\n",
    "    test_data_indices = token2index_dataset(test_ngram_tokens)\n",
    "    \n",
    "    return token2id, id2token, train_data_indices, val_data_indices, test_data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id, id2token, train_data_indices, val_data_indices, test_data_indices = generate_ngram_dataset(\n",
    "    train_data_tokens, val_data_tokens, test_data_tokens, max_n=2, max_vocab_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)\n",
    "learning_rate = 0.01 \n",
    "num_epochs = 2\n",
    "criterion = torch.nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [100/625], Total Steps: 100, Validation Acc: 78.04, Train Acc: 81.4\n",
      "Epoch: [1/2], Step: [200/625], Total Steps: 200, Validation Acc: 82.5, Train Acc: 85.595\n",
      "Epoch: [1/2], Step: [300/625], Total Steps: 300, Validation Acc: 83.96, Train Acc: 89.505\n",
      "Epoch: [1/2], Step: [400/625], Total Steps: 400, Validation Acc: 83.66, Train Acc: 90.79\n",
      "Epoch: [1/2], Step: [500/625], Total Steps: 500, Validation Acc: 83.72, Train Acc: 91.885\n",
      "Epoch: [1/2], Step: [600/625], Total Steps: 600, Validation Acc: 84.48, Train Acc: 92.435\n",
      "Epoch: [2/2], Step: [100/625], Total Steps: 725, Validation Acc: 83.96, Train Acc: 93.45\n",
      "Epoch: [2/2], Step: [200/625], Total Steps: 825, Validation Acc: 83.34, Train Acc: 93.82\n",
      "Epoch: [2/2], Step: [300/625], Total Steps: 925, Validation Acc: 83.4, Train Acc: 94.32\n",
      "Epoch: [2/2], Step: [400/625], Total Steps: 1025, Validation Acc: 83.28, Train Acc: 94.5\n",
      "Epoch: [2/2], Step: [500/625], Total Steps: 1125, Validation Acc: 82.62, Train Acc: 94.975\n",
      "Epoch: [2/2], Step: [600/625], Total Steps: 1225, Validation Acc: 83.16, Train Acc: 93.87\n"
     ]
    }
   ],
   "source": [
    "total_steps = 0 \n",
    "results = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_batch, length_batch, label_batch) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and (i+1) % 100 == 0:\n",
    "            total_steps = epoch * 625 + (i+1) \n",
    "            train_acc = test_model(train_loader, model) # report train accuracy \n",
    "            val_acc = test_model(val_loader, model) # report validation accuracy \n",
    "            result = {} \n",
    "            result['step'] = total_steps \n",
    "            result['train_acc'] = train_acc\n",
    "            result['val_acc'] = val_acc        \n",
    "            results.append(result)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Total Steps: {}, Validation Acc: {}, Train Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), total_steps, val_acc, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
